\documentclass[12pt,a4paper,twoside]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[polish]{babel}
\usepackage{graphicx}
\usepackage{times}
\usepackage{indentfirst}
\usepackage[left=3cm,right=2cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage{natbib}
\usepackage{enumitem}
\usepackage{color}
\usepackage{tikz}
\usepackage{booktabs}
\usepackage{tabulary}
\usepackage{url}
\urlstyle{same}
\setlist{itemsep=0pt}
\setlist{nolistsep}
\frenchspacing
\linespread{1.5}
\addto\captionspolish{%
\renewcommand*\listtablename{Spis tabel}
\renewcommand*\tablename{Tabela}
}
\usepackage{titlesec}
\titlelabel{\thetitle.\quad}
\usepackage{etoolbox}
\makeatletter
\patchcmd{\ttlh@hang}{\parindent\z@}{\parindent\z@\leavevmode}{}{}
\patchcmd{\ttlh@hang}{\noindent}{}{}{}
\makeatother
\setlist[itemize]{leftmargin=*}

\begin{document}

\begin{center}

  \includegraphics[scale=0.3]{../obrazy/sgh_full.png}

  \vspace{1cm}
  Studium magisterskie

\end{center}

\vspace{1cm}

\noindent Kierunek: Analiza danych - big data

\noindent Specjalność: \dots

\vspace{1cm}

{
\leftskip=10cm\noindent
Roni Chikhmous\newline
Nr albumu: 69684

}

\vspace{2cm}

\begin{center}
  \LARGE
  Optymalizacja kosztowa procesu konstruowania głębokich sieci neuronowych (ang. deep neural networks) z wykorzystaniem chmury obliczeniowej
\end{center}

\vspace{1cm}

{
\leftskip=10cm\noindent
Praca magisterska napisana\newline
w Instytucie Ekonometrii\newline
pod kierunkiem naukowym\newline
dr. Przemysława Szufla

}

\vfill

\begin{center}
Warszawa, 2017
\end{center}
\thispagestyle{empty}

\clearpage
\thispagestyle{empty}
\mbox{}

% druga strona będzie pusta, ponieważ drukujemy dwustronnie
\clearpage

\tableofcontents

\clearpage

\section{Wprowadzenie}

% Uzasadnienie podjętego tematu

Ze względu na znaczne postępy w technologiach informatycznych zanotowane w ostatnich latach, przetwarzanie danych w chmurze (ang. \textit{cloud computing}) stało się jednym z wiodących paradygmatów w tej dziedzinie wiedzy. Odnosi się on do aplikacji dostarczanych jako usługi za pośrednictwem Internetu (ang. \textit{Software as a Service, SaaS}), jak również oferowania infrastruktury (ang. \textit{Infrastructure as a Service, IaaS}) i systemów informatycznych (ang. \textit{Platform as a Service, PaaS}). Rozwiązanie to pozwala konsumentom natychmiastowo odpowiadać na zapotrzebowanie bez konieczności inwestowania znacznych nakładów finansowych w rozbudowywanie własnych centrów danych. W szczególności, duża ilość dostępnej natychmiast, dostosowanej do konkretnego zadania mocy obliczeniowej niezbędna jest do przeprowadzenia wielkoskalowych symulacji, estymacji skomplikowanych modeli statystycznych lub algorytmów uczenia maszynowego. Niniejsza praca ma na celu zbadanie i porównanie kosztów budowy modeli głębokich sieci neuronowych (ang. \textit{deep neural networks}) z wykorzystaniem podstawowych serwerów oraz droższych, dedykowanych jednostek, wzbogaconych o procesory graficzne (ang. \textit{graphics processing unit, GPU}), oferowanych przez najpopularniejszych dostawców tego typu usług.

% Ogólny opis:
  %podjętego tematu pracy,
  %postawionych celów,
  %struktury treści,
  %zastosowanej metody analitycznej/badawczej

% Omówienie literatury przedmiotu pracy

\clearpage

\section{Przedstawienie badanych zagadnień}

\subsection{Deep learning - podstawy teoretyczne, zastosowanie}

\clearpage

\subsection{Cloud computing}

% Wstępny opis cloud computingu. Prezentacja obecnie obowiązującego paradygmat
W tym rozdziale omówiony zostanie paradygmat przetwarzania danych w chmurze. Przedstawione zostaną jego kluczowe cechy, wymagania stawiane dostawcom i definicje często spotykanych pojęć. Następnie opiszemy najpopularniejsze modele świadczonych usług, modele implementacji i rozwiązania technologiczne leżące u podstaw opisywanego paradygmatu, by finalnie przejść do aspektów ekonomicznych i przybliżenia rynku \textit{cloud computingu} wraz z opisem największych dostawców.

\subsubsection{Definicje, kluczowe charakterystyki}

\noindent
Przetwarzanie danych w chmurze definiowane jest w pracach \citet{buyya2009} oraz \citet{calheiros2010} jako ,,rodzaj równoległego i rozproszonego systemu, składającego się z licznych, połączonych ze sobą i wirtualizowanych serwerów, która są dynamicznie przydzielane na podstawie ustaleń pomiędzy dostawcą a konsumentami, dokonanych za pośrednictwem dedykowanego serwisu''. Innymi słowy, w tym modelu pamięć, moc obliczeniowa znajdują się ,,w chmurze'', która jest zbiorem centrów danych (ang. \textit{data centers}), posiadanych oraz utrzymywanych przez zewnętrzny podmiot. Konsumenci otrzymują dostęp do owej infrastruktury lub serwisów na niej bazujących przez wystosowanie żądania odwzorowującego ich aktualne potrzeby na tego typu usługi, bez wymagania rezerwacji z dużym wyprzedzeniem czasowym i znacznego nakładu kapitału (ang. \textit{pay-as-you go model}).

To podejście, odnoszące się do świadczenia usług w zakresie dostarczania infrastruktury, mocy obliczeniowej lub aplikacji za pośrednictwem Internetu, stało się jednym z głównych paradygmatów w tej dziedzinie nauki i biznesu. Efektem tego optymizmu jest wizja dostarczania mocy obliczeniowej jako kolejnego rodzaju powszechnie dostępnego medium. Ma ono zaspokajać, ze stosunkowo niewielkim opóźnieniem, rosnące w erze digitalizacji zapotrzebowanie na jednostki będące w stanie wykonywać skomplikowane obliczenia. Leonard Kleinrock, naukowiec, który wniósł istotny wkład w budowę Internetu, twierdził w 1969 jako główny inżynier projektu ARPANET, że: ,,W chwili obecnej, sieci komputerowe są wciąż we wczesnym okresie rozwoju, jednakże wraz z ich dojrzewaniem obserwować będziemy rozpowszechnienie narzędzi komputerowych (ang. \textit{computer utilities}), które jak obecnie elektryczność lub sieci telefoniczne, będą służyć gospodarstwom domowym i przedsiębiorstwom w całym kraju''  \citep{kleinrock2005}. Kleinrock skutecznie przewidział nie tylko mającą nadejść powszechność połączenia internetowego, lecz również możliwość wynajmowania maszyn wirtualnych w sposób zdalny, z natychmiastowym skutkiem, w zależności od aktualnego popytu - tak jak ma to miejsce przypadku każdego innego rodzaju powszechnie dostępnego medium. Również Nicholas Carr porównuje potencjalne konsekwencje nadchodzącej zmiany do tworzenia sieci elektrycznej na początku XX w., por. \citet{carr2008}. Kontynuując tę analogię, opłaty za tego typu usługę naliczane są tylko w wypadku, gdy konsument korzystał z dostępnych mu zasobów. Ponadto, podejście to eliminiuje konieczność utrzymywania własnej infrastruktury informatycznej - rewolucja w szczególności dotyczy sposobu projektowania rozwiązań technologicznych w przedsiębiorstwach, które dotychczas w tej materii opierały się na założeniu pełnej lub częściowej samodzielności centrów danych. Istotność tego typu serwisów podkreśla w swojej pracy również Armburst \citep{armburst2010} określając je jako długo wyczekiwane marzenie dostarczenia przetwarzania danych jako szeroko dostępnego medium, które będzie napędzać transformację przemysłu technologii informacyjnych, czyniąc jednocześnie udostępnianie oprogramowania coraz atrakcyjniejszym rodzajem świadczonych usług.

Oprócz korzyści strukturalnych i biznesowych, ekonomia skali towarzysząca tego typu przedsiewzięciom wpływa na zmniejszenie śladu ekologicznego centrów danych, por. \citet{oecd2014}. Odbywa się to zarówno poprzez zmniejszanie zjawiska określanego jako nadmiernie zaopatrywanie się w zasoby (ang. \textit{over-provisioning}), jak również implementowanie efektywniejszych sposobów przetwarzania danych i tworzenia dedykowanych rozwiązań architektonicznych i instalacyjnych, takich jak systemy chłodzenia. Należy jednak również zwrócić szczególną uwagę na potencjalne zagrożenia związane z wzrastającą popularnością tego typu technologii takich jak bezpieczeństwa i prywatności danych. Zostaną one omówione w późniejszej części rozdziału.

Aby wprowadzić pewne terminy i usystematyzować zbiór pojęć wykorzystywany w niniejszej pracy, oparto się na zestawie rekomendacji wydanym przez Państwowy Instytut Norm i Technologii Stanów Zjednoczonych \citep{mell2011}. Określa on \textit{cloud computing} jako model stworzony w celu umożliwienia wszechobecnego, wygodnego, dostępnego na żądanie, współdzielonego zbioru konfigurowalnych zasobów komputerowych, które mogą zostać natychmiastowo dostarczone przy niewielkim wysiłku zarządzającego i minimalnym stopniu interakcji. Infrastruktura rozwiązań pozwalających na przetwarzanie danych w chmurze jest zestawieniem sprzętu komputerowego oraz oprogramowania, które można opisać jako warstwę fizyczną (sprzęt komputerowy: serwery, dyski twarde i komponenty sieciowe) i warstwę abstrakcyjną (oprogramowanie dostępne na warstwie fizycznej). Model ten cechować musi pięć kluczowych charakterystyk, wyróżnione również są trzy modele świadczonych usług i cztery modele implementacji.

\noindent
Kluczowe charakterystyki:
\begin{itemize}
\item samoobsługowość oraz dostępność na żądanie (ang. \textit{on-demand self-service}) -- klient może jednostronnie wynajmować jednostki obliczeniowe, również w sposób zautomatyzowany,
\item szerokopasmowy dostęp sieciowy (ang. \textit{broad network access}) -- oferowane usługi dostępne są za pośrednictwem sieci, dostęp do nich można uzyskać przez standardowe mechanizmy, z wykorzystaniem zróżnicowanych platform (takich jak laptopy, komputery osobiste, telefony komórkowe czy tablety),
\item błyskawiczna elastyczność (ang. \textit{rapid elasticity}) -- zasoby mogą być w sposób elastyczny rezerwowane i zwalniane, również automatycznie. Z perspektywy konsumenta oferowane zasoby wydają się być nieograniczone,
\item mierzalność usług (ang. \textit{measured service}) -- systemy w sposób automatyczny kontrolują i optymalizują wykorzystanie zasobów dostosowując się do pewnej, uprzednio ustalonej miary (typowo stawki określanej jako płać i korzystaj (ang. \textit{pay-per-use}). Jednocześnie musi być zapewniona pełna transparentność zarówno dla dostawcy usług, jak i konsumenta.
\end{itemize}

\noindent
Modele świadczonych usług :
\begin{itemize}
\item Oprogramowanie jako usługa (ang. \textit{Software as a Service, SaaS}) -- konsument otrzymuje gotowe oprogramowanie, aplikacje, działające w infrastrukturze chmurowej. Do tego typu rozwiązań włącza się różnego rodzaju aplikacje stworzone na potrzeby konkretnych procesów i celów biznesowych. Są one dostępne z różnych rodzajów klientów i urządzeń. Jako przykład tego typu usług zaliczyć można zarówno aplikacje e-mailowe skierowane do odbiorców biznesowych, jak również zintegrowane systemy zarządzania relacjami z klientem. Konsument nie zarządza leżącą u podstaw aplikacji infrastrukturą (w tym siecią, serwerami, systemami operacyjnymi, pamięcią),
\item Platforma jako usługa (ang. \textit{Platform as a Service, PaaS}) -- konsument otrzymuje możliwość wdrażania do infrastruktury chmurowej aplikacji stworzonych z wykorzystaniem wspieranych przez dostawcę języków programowania, bibliotek i narzędzi. Dostawcy tego typu usług posługują się i udostępniają dedykowane interfejsy programistyczne aplikacji (ang. \textit{Application Programming Interface (API)}). Również w tym wypadku konsument nie zarządza infrastrukturą technologiczną,
\item Infrastruktura jako usługa (ang. \textit{Infrastructure as a Service, IaaS}) -- konsument otrzymuje możliwość zarządzania sieciami, pamięcią, jednostkami obliczeniowymi i innymi fundamentalnymi zasobami, mogąc jednocześnie wdrażać i uruchamiać dowolne oprogramowanie, w tym rówież systemy operacyjne i aplikacje. Pozwala to konsumentowi na dużą elastyczność w zaspokajaniu swoich potrzeb informatycznych, nie ma jednak wciąż dostępu do fizycznej infrastruktury.
\end{itemize}

Schemat modeli usług świadczonych w chmurze wraz z porównaniem do tradycyjnych systemów IT zaprezentowany został również na Rysunku \ref{fig:cloudarch}.

\begin{figure}[h]
  \centering
\includegraphics[width=\textwidth, keepaspectratio]{../obrazy/fig:cloudarch.png}
\caption{Stopień ingerencji w oferowane zasoby jest największy w przypadku platformy IaaS, podczas gdy rozwiązania SaaS pozwala użytkownikowi na minimalną inwestycję czasową w zarządzanie infrastrukturą \label{fig:cloudarch}}
\end{figure}

Należy jednak mieć na uwadze, że rozgraniczenie pomiędzy modelem \textit{IaaS} i \textit{PaaS} nie jest aż tak wyraźne jak w przypadku modelu \textit{SaaS}. W niektórych publikacjach \citep{armburst2010} są one traktowane jako grupa rozwiązań, które podzielają więcej podobieństw niż różnic.

\noindent
Modele implementacji:
\begin{itemize}
\item Prywatna chmura (ang. \textit{Private Cloud}) -- infrastruktura zostaje udostępniona na wyłączność dla pojedynczego podmiotu lub organizacji. Może być posiadana i zarządzana przez tę organizację, podmiot trzeci lub dowolną ich kombinację. Może istnieć na terenie dostawcy usług lub poza nim,
\item Zbiorowa chmura (ang. \textit{Community cloud}) -- infrastruktura udostępniona zostaje na wyłączność dla pewnej zbiorowości podmiotów, która mają wspólne cele. Może być posiadana i zarządzana przez jedną lub wiecej z organizacji należących do owej zbiorowości, podmiotu trzeciego lub dowolnej ich kombinacji. Może istnieć na terenie dostawcy usług lub poza nim,
\item Publiczna chmura (ang. \textit{Public cloud}) -- infrastruktura udostępniona dla wszystkich użytkowników. Może być posiadana i zarządzana przez przedsiebiorstwo prywatne lub publiczne, placówkę akademicką lub dowolną ich kombinację. Musi istnieć na terenie dostawcy usług,
\item Hybrydowa chmura (ang. \textit{Hybrid cloud}) -- infrastruktura jest połączeniem dowolnych dwóch lub więcej infrastruktur chmurowych, które pozostają odrębnymi bytami, lecz są związane rozwiązaniami technologicznymi. W przypadku połączenia prywatnej i publicznej chmury pozwala to na utrzymywanie dedykowanej infrastruktury, stanowiącej rdzeń systemu informatycznego przedsiębiorstwa, która w razie wyjątkowego obciążenia systemu lub awarii może zostać wspomagana zasobami pochodzącymi z chmury publicznej, por. \citet{antonopoulos2010}.
\end{itemize}

Ponieważ wiele prywatnych centrów danych korzysta z terminu ,,przetwarzania w chmurze'' w celu opisania swojego modelu biznesowego i przejaskrawienia potencjalnych korzyści, \citet{armburst2010} silnie podkreślają różnicę pomiędzy stosowanymi przez takich dostawców rozwiązaniami, a infrastrukturą posiadającą wszystkie wskazane cechy. Uargumentowane jest to faktem, że ekonomia skali i możliwość błyskawicznego dostosowywania się do zmian w popycie jest w przypadku tego przedsięwzięcia kluczowym czynnikiem, fundamentalnym dla jego opłacalności. Małej i średniej wielkości centra danych (nie posiadające setek tysięcy-milionów serwerów) nie mogą oferować swoim klientom atrakcyjniejszych cen, ponieważ czerpią tylko z podzbioru wymienionych wcześniej możliwości. Ponadto, jak twierdzi \defcitealias{mustafa2015}{Mustafa (2015)}\citetalias{mustafa2015}, paradygmat \textit{cloud computingu} jest w swojej naturze skierowany na potrzeby rynku, podczas gdy klasyczna architektura jest systemocentryczna i nie daje dostawcy bodźców do traktowania wszystkich zgłoszeń z równą wagą. Z wymienionych powodów prywatny model chmury często nie jest przywoływany w debatach nad tego typu rozwiązaniami.

Na tym etapie warto również przywołać pokrewne technologie, takie jak \textit{grid computing} oraz \textit{utility computing}, por. \citet{zhang2010}. \textit{Grid computing}, zwany również przetwarzaniem sieciowym jest paradygmatem rozproszonego przetwarzania danych, który koordynuje zasoby znajdujące się w jednej sieci, by osiągnąć jeden ustalony cel w przetwarzaniu. To rozwiązanie ma swoje korzenie w środowisku naukowym, gdzie wykonywane obliczenia potrzebowały dużej ilości zasobów. Różnica pomiędzy nim a przetwarzaniem w chmurze polega na stopniu wykorzystywania technologii wirtualizacji na wielu poziomach (sprzętowym i aplikacyjnym). Druga z wymienionych technologii, \textit{utility computing}, reprezentuje model dostarczania zasobów na żądanie i fakturowaniu klientów na podstawie ich wykorzystania zamiast stałej stawki. Przetwarzanie w chmurze może być rozpatrywane jako pewnego rodzaju realizacja \textit{utility computingu}. Adaptuje ona schemat ustalania stawek ze względów czysto ekonomicznych.

\subsubsection{Wirtualizacja i zarządzanie zasobami}

\noindent
U podstaw przedstawianej infrastruktury leżą dwie technologie: wirtualizacja oraz zarządzanie zasobami \citep{mustafa2015}. Techniki wirtualizacyjne wykorzystywane są w celu zapewnienia elastycznego i dynamicznego przydzielania mocy obliczeniowej, podczas gdy proces zarządzanie zasobami odpowiada za ich dostarczanie i nadzór nad nimi. Dzieje się to poprzez dzielenie jednej fizycznej maszyny na wiele logicznych systemów operacyjnych z wykorzystaniem hipernadzorcy (ang. \textit{hypervisor}), który emuluje leżący u podstaw model sprzętowy i odpowiada za inicjalizację i nadzorowanie pracy wielu systemów operacyjnych. Rysunek \ref{fig:vm}. przedstawia działanie takiego typu architektury i porównuje go do tradycyjnego podejścia. Wirtualizowany serwer powszechnie nazywany jest maszyną wirtualną (ang. \textit{Virtual Machine, VM}). Cykl życia maszyny wirtualnej obejmuje sześć faz: tworzenie, zawieszenie, wznowienie, zapisanie, migrację oraz niszczenie.

Wirtualizacja po raz pierwszy została wykorzystana na wielką skalę przez IBM w latach 1960 przy systemach klas komputerów określanych jako komputery głównego szeregu (ang. \textit{mainframe systems}). Środowiska wirtualne i systemy operacyjne używane na tych komputerach pozwalały na uruchamianie wielu aplikacji i procesów dla wielu użytkowników jednocześnie i utrzymanie wysokiego poziomu automatyzacji przydzielania zadań. Robert P. Goldberg w 1974 roku opisał potrzebę wykorzystywania maszyn wirtualnych w następujący sposób: ,,Systemy maszyn wirtualnych były oryginalnie zbudowane by naprawić pewne wady architektur trzeciej generacji i wieloprogramowych systemów operacyjnych (ang. \textit{multiprogramming operating systems}, takich jak OS/360'' \citep{goldberg1974}. W latach 1980 i 1990 dominującym podejściem do przetwarzania danych były systemy rozproszeone, aplikacje klient-serwer oraz tanie serwery x86. Osiagnięcia technologiczne ostatnich lat doprowadziły jednak do znacznego wzrostu wydajności stosowania tego typu rozwiązań i wykorzystania wielordzeniowych procesorów na szeroką skalę. Opracowane niedawno, popularne architektury sprzętowe, takie jak rodzina architektur procesorów Intel X86 czy AMD-V silnie czerpią z tego modelu (przykładem mogą również być takie implementacje jak VMware, Xen czy KVM, por. \citet{shroff2010}). Powrót do tego podejścia argumentowany jest także oszczędnością energii \citep{ward2013}, znacznie większą stabilnością tak zaprojektowanego systemu, a w przypadku przedsiębiorstw oferujących usługi przetwarzania w chmurze również możliwością dostosowywania się do błyskawicznie zmieniających się potrzeb konsumentów. Przykładowo w przypadku awarii jednej z maszyn, wszystkie wykonywane przez nią zadania automatycznie zostają migrowane na jedną lub wiele pozostałych jednostek. Ponadto, dostawcy muszą mieć pewność, że są wystarczająco elastyczni, by w czasie rzeczywistym zaspokajać popyt na różne rodzaje jednostek, izolując jednocześnie w wymaganym stopniu użytkowników od fizycznej infrastruktury. To podejście do zarządzania zasobami, ich stałe monitorowanie i możliwość automatycznego reagowania na awaryjne sytuacje muszą być zapewnione w przypadku środowiska \textit{cloud computingowego} i są jego inherentną częścią.

\begin{figure}[h]
  \centering
\includegraphics[scale=0.8]{../obrazy/fig:vm.png}
\caption{Wirtualizacja pozwala na uruchomienie licznych maszyn wirtualnych na jednej maszynie fizycznej z wykorzystaniem hipernadzorcy (w tym wypadku \textit{VMware}) \label{fig:vm}}
\end{figure}

Technologia wirtualizacji musi jednak zmierzyć się z pewnymi wyzwaniami. Architektura x86 nie była pierwotnie tworzona jako platforma dedykowana tym rozwiązaniom. Mechanizmy, które pozwalają na tworzenie maszyn wirtualnych na tego typu architekturze wymagają zmodyfikowanego systemu operacyjnego lub używają dodatkowego zestawu instrukcji, dostarczonego przez nowoczesne procesory. Z tego powodu wykorzystywanie wirtualizacji wiąże się z pewnym spadkiem wydajności. Choć według \citet{menon2005} został on w ostatnich latach zminimalizowany, wirtualne maszyny wciąż dostarczają tylko ułamek wydajności równoważnej fizycznej maszyny. Niektóre rodzaje hipernadzorców pozwalają na dostarczenie niemal natywnej wydajności procesora, jednak sedno problemu leży w wydajności procesów wejścia/wyjścia (ang. \textit{input/output (I/O)}), którego spowolnienie może osiągać nawet 88\% w porównaniu do maszyny fizycznej. Ponadto, należy skupić się także na minimalizowaniu strat w przypadku uruchamianie wielu maszyn wirtualnych na jednym rdzeniu procesora. Ponieważ w danym momencie na jednym procesorze może być aktywna tylko jedna maszyna wirtualna, pozostałe z nich pozostają bezczynne i nie mogą odpowiadać na aktywność \textit{I/O}.

% wincyj

\subsubsection{Aspekty ekonomiczne}

Przetwarzanie w chmurze stało się jedną z najszybciej rozwijających się gałęzi technologii informacyjnych. W 2009 r. sprzedaż w usługach chmurowych osiągnęła 56 miliardów dolarów \citep{smith2009}, podczas gdy Merryl Lynch przewidywał wzrost wartości rynku do 160 miliardów w 2011. Raport Gartner z 2016 roku \citep{anderson2016} wskazuje, że rynek platform BPaaS, SaaS, Paas oraz Iaas osiągnął w sumie wartość 734 miliardów dolarów, przy czym kolejne 111 miliardów zostanie w niego zainwestowana do 2020 r. Ponieważ uważa się, że technologia ta ma wszelkie argumenty, by fundamentalnie i na stałe zmienić sposób zarządzania zasobami informatycznymi przedsiębiorstwa, dyrektorzy i menedżerowie IT powinni nieustannie śledzić rozwój tej innowacji i dostosowywać do niej swoje długofalowe strategie.

Sukces \textit{cloud computingu} ściśle związany jest z korzyściami ekonomicznymi, które za sobą niesie. Rozwiązanie to pozwala na obniżenie kosztów obsługi, szybszą implementację i większą elastyczność w wykorzystywaniu różnych zestawów narzędzi. Wykorzystanie modeli \textit{pay-as-you-go} skutkuje także minimalizowaniem ryzyka nadmiernego gromadzenia zasobów (ang. \textit{overprovisioning}), jak i ich niewystarczającej ilości (ang. \textit{underprovisioning}). Według \citet{armburst2010} średnie wykorzystanie serwerów w centrach danych wynosi od 5\% do 20\%, co jest spójne z obliczeniami, według których największe obciążenie serwerów przekracza nawet dziesięciokrotnie średnią.  W tym modelu dodatkowe maszyny mogą zostać usunięte lub dodane w ciągu kilku minut (w przeciwieństwie do tygodni-miesięcy w przypadku klasycznego modelu). Efektem tego jest zbilansowana proporcja zapotrzebowania na sprzęt i faktycznego stanu posiadania, co minimalizuje koszty i pozwala uniknąć przeciążania serwerów (zjawisko \textit{overprovisioningu} i \textit{underprovisioningu} zostało zilustrowane na Rysunku \ref{fig:overp}.). Największe korzyści z tego rozwiązania mogą uzyskać te przedsiebiorstwa, które z wykorzystaniem analiz ekonometrycznych potrafią przewidzieć zapotrzebowanie na swoje usługi (tym bardziej jeśli zapotrzebowanie dynamicznie zmienia się na przykład w zależności od pory dnia lub online-owych kampanii reklamowych, jak ma to miejsce w przypadku e-commerce). Ponadto, ta cecha \textit{cloud computingu} ma silny wpływ na jego popularność wśród małych i średnich przedsiębiorstw, które są we wczesnym etapie rozwoju. Promuje ona innowacyjne rozwiązania i pozwala na łatwe znalezienie zasobów koniecznych do ich realizacji. Dlatego też nawet jeśli nabywanie serwerów w modelu \textit{pay-as-you-go} okazuje się droższe niż metoda tradycyjna, może to być atrakcyjniejszą alternatywą ze względu na możliwość szybkiego zwolnienia maszyn bez dodatkowych kosztów.

\begin{figure}[h]
  \centering
\includegraphics[scale=0.8]{../obrazy/fig:overp.png}
\caption{Tradycyjna architektura wymusza rzadkie zmiany, które skutkują częstym nadmiarem zasobów obliczeniowych. Może on być nawet pogłębiony przez nieoczekiwany spadek zapotrzebowania tuż po zakupieniu dodatkowej mocy obliczeniowej, co generuje dodatkowe koszty. \label{fig:overp}}
\end{figure}

Korzystając z gotowych rozwiązań oferowanych przez dostawców usług \textit{cloud computingowych} klienci mogą również liczyć na zdecydowanie wyższy poziom bezpieczeństwa niż dotychczas, ponieważ ich dane chronione są przez jedno, wysoce wyspecjalizowane w tym zakresie przedsiębiorstwo.

Tego rodzaju model jest także bardzo opłacalny dla dostawców, ponieważ sposób zarządzania tego rodzaju homogenicznym systemem może być optymalizowany. Według \citet{baun2011} konstruowanie wielkoskalowych rozwiązań chmurowych w tanich lokalizacjach pozwala na obniżenie kosztów elektryczności, połączenia sieciowego, operacji, oprogramowania i sprzętu nawet siedmiokrotnie.

Pojawienie się przetwarzania w chmurze miało stymulujący wpływ na gospodarkę, ale także stawia przed sobą wiele nowych wyzwań. W związku z tym, regulatorzy mają również istotną rolę do odegrania w wielu obszarach:
\begin{itemize}
\item bodźce do używania technologii chmurowych -- rządy muszą odegrać rolę w zachęcaniu wykorzystania \textit{cloud computingu} na przykład poprzez usuwanie zbędnych barier prawnych i regylacyjnych, ale również przez bycie wiodącym ich użytkownikiem i zachęcanie do partnerstw publiczno-prywatnych,
\item standardy -- jednym z wielu wyzwań stojących przed rozwojem chmury jest brak odpowiednich standardów w niektórych obszarach, jak również niedostateczne przyjęcie istniejących standardów. Rządy powinny zachęcać i wspierać rozwój otwartych standardów dla wsparcia biznesu, także poprzez współpracę z różnym instytutami normalizacyjnymi,
\item pomiary -- ilość dostępnych publicznie danych o tego typu przedsięwzięciach jest wciąż niewielka. Regulatorzy, w konsultacji z udziałowcami, powinni odegrać istotną rolę w identyfikowaniu stosownych kryteriów pomiaru wyników i oceny stanu tej gałęzi rynku.
\item kraje rozwijające się -- \textit{cloud computing} daje szansę korzystania ze swoich korzyści przy niskich kosztach dla organizacji i konsumentów pochodzących z krajów rozwijających się. Należy zapewnić odpowiednią infrastrukturę sieciową oraz zachęcać do adaptacji i wykorzystania nowych technologii w owych krajach tak, by mogły one doświadczyć wzrostu ekonomicznego, poprawić zdolności edukacyjne, ale również by umożliwić wolny przepływ informacji dla rozwoju społeczeństwa,
\item konkurencja i handel -- ustawodawcy muszą zadbać o to, by nie dopuścić do niekonkurencyjnych praktyk wynikających z dominacji rynkowej kilku niewielkich przedsiębiorstw. Jest to niezwykle istotne, ponieważ (przywołując również teorię \textit{cloud computingu} jako medium) gałęzie rynku, gdzie kluczowym czynnikiem jest ekonomia skali są szczególnie narażone na tego typu sytuację,
\item podatki -- wraz z migracją wielu centrów danych do chmury, należy rozważyć potencjalny wpływ na rozliczenia podatkowe związane z dostarczaniem tych usług. Szczególnie ważna jest to w przypadku przedsiębiorstw, które dotychczas miały niewielkie doświadczenie w transgranicznych transakcjach i rachunkowości,
\item bezpieczeństwo i zarządzanie ryzykiem -- należy wprowadzić regulacje, które pozwolą trafnie zdefiniować stojące przed \textit{cloud computingiem} wyzwania związane z bezpieczeństwem infrastruktury, w szczególności konieczne jest przeprowadzenie oceny ryzyka, zarządzanie bezpieczeństwem i w efekcie jego usprawnienie,
\item prywatność -- regulatorzy powinni zaadresować pojawiające się pytania dotyczące prywatności przechowywanych danych, takich jak: ,,prawa którego państwa stosuje się do danych przechowywanych w chmurze?'', ,,kto ma dostęp do przechowywanych danych?'', ,,w jakich sytuacjach dane mogą zostać udostępnione jednostkom rządowym?''.
\end{itemize}

\subsubsection{Wiodący dostawcy usług \textit{cloud computingowych}}

\noindent
Według najnowszego raportu ,,Magic Quadrant'' firmy Gartner \citep{leong2017}, liderem rynku pozostaje, niezmiennie od 2006 roku, Amazon Web Services, na kolejnych pozycjach plasują się Microsoft, IBM oraz Google (wyniki rankingu przedstawione zostały na Rysunku \ref{fig:mquad}.). Według \citet{forbes2017}, Amazon posiada 31\% udziału w rynku, Microsoft 11\%, podczas gdy IBM i Google odpowiednio 7\% i 5\%. Jednak, pomimo statusu lidera, Amazon musi czynić intensywne wysiłki by zachować swoją pozycję, ponieważ inni wielcy gracze rozwijają się w imponującym tempie, zdobywając coraz większy udział w rynku infrastruktury chmurowej. Microsoft w czwartym kwartale 2015 powiększył swoje obroty o 124\%, podczas gdy w drugim kwartale 2016 roku to Google odnotował największe tempo wzrostu (162\%). Należy jednocześnie zwrócić uwagę na fakt, że czterech największych graczy stopniowo zwiększa swój udział w rynku (z 51\% w czwartym kwartale 2015 do 54\% w drugim kwartale 2016), por. \citet{forbes2017}.

\begin{figure}[h]
  \centering
\includegraphics[scale=0.8]{../obrazy/fig:mquad.png}
\caption{\textit{Magic Quadrant} dla przedsiębiorstw oferujących usługi \textit{cloud computingowe} potwierdza status lidera AWS. Należy jednak rozpatrywać Microsoft oraz Google jako równorzędnych konkuretnów. \label{fig:mquad}}
\end{figure}

Amazon, pierwszy dostawca usług przetwarzania w chmurze, przechodził stopniową transformację od portalu oferującego książki do online'owego potentata, wykorzystując jednocześnie wysoce innowacyjne rozwiązania do osiągnięcia tego celu. Ta otwartość na innowacje była niejako wymuszona stopniem skomplikowania oferowanego serwisu. Wyświetlenie jednej strony zawierającej informacje o przedmiocie, rekomendacje oraz oceny wymagało licznych, skomplikowanych aplikacji. Ponadto, detaliczny charakter sprzedaży wymagał ciągłego monitorowania zapotrzebowania na zasoby obliczeniowe, ale również obserwowania popytu i wsparcia  procesów operacyjnych systemami informatycznymi (zarówno swoich, jak i partnerów biznesowych). Amazon wykorzystywał technologię wirtualizacji podobnie, jak wiele innych przedsiębiorstw obecnie, by automatycznie i dynamicznie przydzielać zasoby do aplikacji, które tego potrzebowały w danej chwili. Wysoki stopień automatyzacji, który osiągneli w tym zakresie pozwolił im na uruchomienie ich dwóch największych serwisów -  Amazon EC2 (Elastic Compute Cloud) oraz Amazon S3 (Simple Storage System). Amazon EC2 pozwala użytkownikom chmury uruchamiać serwery i zarządzać nimi w jednym z wielu umiejscowionych na różnych kontynentach centrów danych, podczas gdy Amazon S3 jest usługą służącą do przechowywania danych w chmurze. Usługi te miały początkowo zostać udostępnione wewnętrznie i dla partnerów biznesowych, jednak w ramach eksperymentu otwarto je również dla całego świata, gdzie spotkały się z ogromną popularnością, por. \citet{shroff2010}. Ostatecznie doprowadziło do decyzji o uruchomieniu biznesu \textit{cloud computingowego} w 2006 roku. Od tego czasu AWS znacznie rozszerzyło gamę oferowanych usług i dostarcza swoje usługi z wykorzystaniem centrów danych znajdujących się w Ameryce Północnej, Europie, Azji oraz Ameryce Południowej. Do ich silnych stron zaliczana jest duża liczba klientów, najszerszy wachlarz oferowanych usług oraz największe możliwości obliczeniowe, co doprowadziło do ich partnerstwa technologicznego z licznymi przedsiębiorstwami, które dostosowały swoje produkty do uruchamiania na platformie AWS. Z drugiej strony, wymaga to od początkującego użytkownika dużego nakładu czasu, by zaznajomić się z modelem działania i oferowanymi usługami tak, by mógł dostosować je do swoich potrzeb technologicznych.

Microsoft także coraz bardziej skupia się na dostarczaniu swoich możliwości technologicznych z wykorzystaniem usług chmurowych. Platforma przez nich Azure, pozwala na wynajęcie maszyn wirtualnych (z wykorzystaniem oprogramowania Hyper-V), przechowywanie obiektów w chmurze, jak również wiele innych możliwości IaaS oraz PaaS. Ponadto, Azure Marketplace oferuje oprogramowanie i usługi dostarczane przez inne podmioty. Co więcej, Microsoft błyskawicznie wprowadza nowe usługi i funkcjonalności, nakreślając wizję połączonych modeli IaaS oraz PaaS, które współdziałać będą z istniejącą infrastrukturą \textit{on-premises}.  Centra danych Microsoft (określane mianem ,,regionów'') znajdują się nie tylko w Ameryce Północnej i Południowej, Europie i Azji, ale także w Australii. Wartym odnotowania jest również fakt otwarcia się na technologie open source - gama tego typu rozwiązań wspieranych przez Azure wciąż się poszerza, ze szczególnym uwzględnieniem systemów operacyjnych Linux. Microsoft na pewno będzie również korzystał z istniejących kontaktów biznesowych oraz silnej marki, by promować swój produkt i osiągnąć jeszcze większy udział w rynku technologii chmurowych. Choć Azure daleko do stopnia rozwoju i dojrzałości prezentowanego przez AWS, wiele organizacji określa go jako wystarczająco rozbudowany, by móc w niego inwestować.

IBM, jeden z najstarszych koncernów informatycznych, również oferuje usługi \textit{cloud computingowe}. W celu przyspieszenia ekspansji na rynku, w 2013 roku IBM kupiło SoftLayer, niezależnego dostawcę usług chmurowych, by w efekcie ukształtować IBM Cloud Services Division. Przez kolejne lata następowała stopniowa migracja klientów z IBM SmartCloud Enterprise do SoftLayer. Obecnie oferują zarówno wirtualizowane serwery, jak i klasyczne dedykowane maszyny. Ponadto, istnieje także możliwość przechowywania obiektów poprzez zintegrowany CDN (ang. \textit{Content Delivery Network}. Softlayer oferuje swoje usługi z wykorzystaniem centrów danych znajdujących się w Ameryce Północnej, Europie oraz Azji. Do zalet IBM Softlayer, podobnie jak w przypadku, Microsoft, można zaliczyć silną markę oraz liczne kontakty z klientami na całym świecie, co z pewnością pozwoli na rozwój w segmencie przedsiębiorstw. Należy jednak zaznaczyć, że historycznie Softlayer był głównie skupiony na klasycznym modelu dostarczania mocy obliczeniowej i nie rozróżnia na swoim portalu serwisów chmurowych i niechmurowych, brak jest także wielu możliwości technologicznych charakterystycznych dla przetwarzania w chmurze, oferowanych przez liderów rynku.

Google operuje na rynku PaaS pod 2008 roku (oferując swój App Engine), jednakże w modelu IaaS aktywni są dopiero od udostępnienia światu Google Compute Engine w 2013 roku. Obecnie w swojej ofercie łączą zarówno wymienione usługi, jak również wachlarz dodatkowych możliwości, takich jak możliwość przechowywania danych i Container Engine - usługę pozwalającą na konteneryzację, to znaczy uruchomienie aplikacji w wydzielonym konetenerze, ale bez konieczności emulowania warstwy sprzętowej i systemu operacyjnego. Centra danych Google znajdują się w Ameryce Północnej, Europie i Azji. Przez wiele lat prowadzenia własnego biznesu internetowego, Google nabyło szeroką ekspertyzę w projektowaniu tego typu rozwiązań. Niewątpliwą zaletą korzystania z ich serwisu jest możliwość korzystania z ich osiągnięć technologicznych w zakresie wirtualizacji, wykorzystania kontenerów czy też narzędzi do zarządzania klastrami jak Kubernetes. Należy wszakże zaznaczyć, że zakres usług przez nich oferowany nie jest jeszcze tak szeroki jak w przypadku liderów rynku. Według \citet{leong2017}, pomimo szybkiego tempa wzrostu, Google wciąż nie rozwija się dostatecznie dynamicznie, by móc osiągnąć status lidera w tej gałęzi rynku informatycznego.

%podsumowanko



\clearpage

\begin{thebibliography}{99}
\setlength{\itemsep}{0pt}%
\bibitem[Anderson i Warrilow (2016)]{anderson2016} Anderson, E. i Warrilow, M. (2016). Market Insight: Cloud Shift — The Transition of IT Spending From Traditional Systems to Cloud. Baza danych Gartner
\bibitem[Antonopoulos i Gilliam (2010)]{antonopoulos2010} Antonopoulos, N. i Gillam, L. (2010). Cloud computing, 1st ed. London: Springer, s. xx - xx
\bibitem[Armburst et al. (2010)]{armburst2010} Armbrust, M., Stoica, I., Zaharia, M., Fox, A., Griffith, R., Joseph, A., Katz, R., Konwinski, A., Lee, G., Patterson, D. i Rabkin, A. (2010). A view of cloud computing. Communications of the ACM, 53(4), s. 50
\bibitem[Baun i Kunze (2011)]{baun2011} Baun, C. i Kunze, M. (2011). Cloud computing. 1st ed. Heidelberg [etc.]: Springer, s. xx -xx
\bibitem[Buyya et al. (2009)]{buyya2009} Buyya, R., Yeo, C., Venugopal, S., Broberg, J. i Brandic, I. (2009). Cloud computing and emerging IT platforms: Vision, hype, and reality for delivering computing as the 5th utility. Future Generation Computer Systems, 25(6), s. 599-616
\bibitem[Carr (2008)]{carr2008} Carr, N. G. (2008). The big switch: Rewiring the world, from Edison to Google. WW Norton and Company, s xx - xx
\bibitem[Calheiros et al. (2010)]{calheiros2010} Calheiros, R., Ranjan, R., Beloglazov, A., De Rose, C. i Buyya, R. (2010). CloudSim: a toolkit for modeling and simulation of cloud computing environments and evaluation of resource provisioning algorithms. Software: Practice and Experience, 41(1), s. 23-50
\bibitem[Forbes.com (2017)]{forbes2017} Forbes.com. (2017). Amazon Continues To Gain Share In Cloud Infrastructure Services Market. Dostępne pod adresem: \url{https://www.forbes.com/sites/greatspeculations/2016/08/17/amazon-continues-to-gain-share-in-cloud-infrastructure-services-market/#2277485215b8} (dostęp 11 marca 2017)
\bibitem[Goldberg (1974)]{goldberg1974} Goldberg, R. P. (1974), Survey of virtual machine research. IEEE Comput Mag 7(6):34–45
\bibitem[Kleinrock (2005)]{kleinrock2005} Kleinrock, L., A Vision for the Internet. ST Journal for Research, tom 2, nr 1, s. 4–5
\bibitem[Leong et al. (2017)]{leong2017} Leong, L., Petri, G., Gill, B. i Dorosh, M. (2017). Magic Quadrant for Cloud Infrastructure as a Service. Gartner.com. Dostępne pod adresem: \url{https://www.gartner.com/doc/reprints?id=1-2G2O5FC&ct=150519} (dostęp 11 marca 2017)
%\bibitem[Lin et al. (2017)]{lin2017} Lin, W., Xu, S., He, L. i Li, J. (2017). Multi-resource scheduling and power simulation for cloud computing. Information Sciences, 397-398, s.168-186.
%\bibitem[Mahmood (2013)]{mahmood2013} Mahmood, Z. (2013). Cloud computing. 1st ed. London: Springer, s. xx - xx
\bibitem[Mell i Grance (2011)]{mell2011} Mell, P. i Grance T. (2011). The NIST definition of cloud computing
\bibitem[Menon et al. (2005)]{menon2005} Menon, A., Santos, J., Turner, Y., Janakiraman, G. i Zwaenepoel, W. (2005). Diagnosing performance overheads in the xen virtual machine environment. Proceedings of the 1st ACM/USENIX international conference on Virtual execution environments - VEE '05.
\bibitem[Mustafa et al. (2015)]{mustafa2015} Mustafa, S., Nazir, B., Hayat, A., Khan, A. i Madani, S. (2015). Resource management in cloud computing: Taxonomy, prospects, and challenges. Computers and Electrical Engineering, 47, s. 186-203
\bibitem[OECD (2014)]{oecd2014} OECD (2014). Cloud Computing: The Concept, Impacts and the Role of Government Policy. OECD Digital Economy Papers, No. 240, OECD Publishing, Paris
\bibitem[Shroff (2010)]{shroff2010} Shroff, G. (2010). Enterprise cloud computing. 1st ed. Cambridge: Cambridge University Press, s. xx - xx
\bibitem[Smith et al. (2009)]{smith2009} Smith D. M. et al. (2009). Hype Cycle for Cloud Computing (ID: G00168780). Baza danych Gartner
\bibitem[Ward i Baker (2013)]{ward2013} Ward, J.S. i Barker, A. (2013). A Cloud Computing Survey: Developments and Future Trends in Infrastructure as a Service Computing
%\bibitem[Trovati (2015)]{trovati2015} Trovati, M. (2015). Big-data analytics and cloud computing. 1st ed. London: Springer, s. xx - xx
\bibitem[Zhang et al. (2010)]{zhang2010} Zhang, Q., Cheng, L. i Boutaba, R. (2010). Cloud computing: state-of-the-art and research challenges. Journal of Internet Services and Applications, 1(1), s. 7-18.

\end{thebibliography}
\clearpage

\listoffigures

\clearpage

\listoftables

\clearpage

\end{document}
