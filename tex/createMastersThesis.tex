\documentclass[12pt,a4paper,twoside]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[polish]{babel}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{times}
\usepackage{indentfirst}
\usepackage[left=3cm,right=2cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage{natbib}
\usepackage{enumitem}
\usepackage{color}
\usepackage{tikz}
\usepackage{booktabs}
\usepackage{tabulary}
\usepackage{url}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{etoolbox}
\usepackage{titlesec}
\urlstyle{same}
\setlist{itemsep=0pt}
\setlist{nolistsep}
\frenchspacing
\linespread{1.5}
\addto\captionspolish{%
\renewcommand*\listtablename{Spis tabel}
\renewcommand*\tablename{Tabela}
}
\makeatletter
\renewcommand{\ALG@name}{Algorytm}
\makeatother
\titlelabel{\thetitle.\quad}
\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatletter
\patchcmd{\ttlh@hang}{\parindent\z@}{\parindent\z@\leavevmode}{}{}
\patchcmd{\ttlh@hang}{\noindent}{}{}{}
\makeatother
\setlist[itemize]{leftmargin=*}

\begin{document}

\begin{center}

  \includegraphics[scale=0.3]{../obrazy/sgh_full.png}

  \vspace{1cm}
  Studium magisterskie

\end{center}

\vspace{1cm}

\noindent Kierunek: Analiza danych - big data

\noindent Specjalność: \dots

\vspace{1cm}

{
\leftskip=10cm\noindent
Roni Chikhmous\newline
Nr albumu: 69684

}

\vspace{2cm}

\begin{center}
  \LARGE
  Optymalizacja kosztowa procesu konstruowania głębokich sieci neuronowych (deep neural networks) z wykorzystaniem chmury obliczeniowej
\end{center}

\vspace{1cm}

{
\leftskip=10cm\noindent
Praca magisterska napisana\newline
w Instytucie Ekonometrii\newline
pod kierunkiem naukowym\newline
dr. Przemysława Szufla

}

\vfill

\begin{center}
Warszawa, 2017
\end{center}
\thispagestyle{empty}

\clearpage
\thispagestyle{empty}
\mbox{}

% druga strona będzie pusta, ponieważ drukujemy dwustronnie
\clearpage

\tableofcontents

\clearpage

\section{Wprowadzenie}
\noindent
Celem niniejszej pracy jest zaproponowanie metody wyznaczania optymalnego kosztowo sposobu generacji struktur głębokich sieci neuronowych (\textit{deep neural networks}). Uczenie tego typu modeli wymaga znacznej mocy obliczeniowej, dostępnej na żądanie i na krótki czas. To sprawia, że tradycyjne rozwiązania (takie jak architektura \textit{on premises}) nie są optymalne kosztowo, por. \citet{armburst2010}, \citet{oecd2014}. Rozwiązaniem, które odpowiada na wymienione potrzeby jest model \textit{pay-as-you-go} wykorzystywany w usługach przetwarzania w chmurze (\textit{cloud computing}).

Odkrycia naukowe ostatnich lat w dziedzinach uczenia maszynowego i sztucznej inteligencji wpłynęły pozytywnie na ich popularność i liczbę potencjalnych zastosowań \citep{lecun2015}. Szczególna uwaga poświęcana jest sieciom neuronowym, które, choć pozwalają na rozwiązywanie dotychczas problematycznych zagadnień \citep{lecun2015}, są algorytmami wymagającymi znacznej ilości danych i mocy obliczeniowej, por. \citet{krizhevsky2012} i \citet{srivastava2014}. Ośrodki naukowe we współpracy z największymi firmami technologicznymi, jak Google czy Microsoft wykorzystują owe modele \citep{goodfellow2016} do rozwiązywania szeregu problemów takich jak klasyfikacja obrazów (\citet{krizhevsky2012}; \citet{shetty2016}; \citet{szegedy2014}; \citet{chen2016}) i przetwarzanie języka naturalnego \citep{hinton2012}. Kluczowa zaleta głębokich sieci neuronowych polega na możliwości budowania modelu z wykorzystaniem nieprzetworzonych danych. W przypadku klasycznych algorytmów \textit{machine learningowych}, konieczna jest inżynieria cech i ręczna ich ekstrakcja, podczas gdy wielowarstwowe sieci neuronowe są metodą, która w sposób automatyczny wykrywa występujące w zbiorach danych schematy, zależności, por. \citet{girschick2014}, \citet{gysel2016} i \citet{mnih2013}. Przykładowo, w przypadku klasyfikacji obrazów, pierwsza warstwa sieci neuronowej odpowiada za detekcję ogólnych jego cech, takich jak umiejscowienie krawędzi w kluczowych miejscach. Kolejna odpowiada za analizowanie ich położenia wzgledem siebie i wykrywanie pewnych motywów. Co do zasady, pierwsze warstwy odpowiadają za ogólne cechy przetwarzanych danych, podczas gdy każda kolejna ma za zadanie przechodzić na wyższy poziom szczegółowości.


Na popularności zyskało wykorzystywanie procesorów graficznych (\textit{Graphics Processing Unit, GPU}) oferowanych przez NVIDIA do poprawy wydajności procesu generacji tego typu struktur (\citet{jermain2015}; \citet{litvinenko2014}; \citet{strom2015}; \citet{vanhoucke2011}). Ponadto, giganci technologiczni inwestują w poszukiwanie potencjalnych sposobów na dalsze usprawnienie tego procesu, takich jak wykorzystanie bezpośrednio programowalnych macierzy bramek (\textit{Field-Programmable Gate Array}) czy rozwiązania opracowanego przez Wave Computing - \textit{Dataflow Processing Unit (DPU)}, por. \citet{gysel2016}; \citet{chen2016}; \citet{han2016}.

Zarówno przeprowadzanie symulacji w chmurze, wykorzystywanie procesorów graficznych w celu poprawy wydajności generacji głębokich sieci neuronowych, jak i porównania poszczególnych jednostek były przedmiotem badań, por. \citet{calheiros2010}, \citet{github2017}, \citet{medium2017b}, \citet{hackernoon2017}. Nie przeprowadzono jednak analizy oszczędności czasowych wynikających ze stosowania \textit{GPU} w chmurze i brak jest odpowiedzi na pytanie, czy rekompensują one ich wyższą cenę w porównaniu do klasycznych jednostek.

Zatem, w pracy badane będą następujące hipotezy:
\begin{itemize}
\item stosowanie jednostek wykorzystujących GPU jest rozwiązaniem efektywnym kosztowo  w porównaniu do jednostek wyposażonych wyłącznie w CPU,
\item wynajem jednostek AWS P2 jest efektywniejsze kosztowo od jednostek oferowanych przez Microsoft Azure oraz Google Cloud,
\item wykorzystywanie opracowanego narzędzia do wyboru rodzaju jednostki obliczeniowej pozwala na optymalizację kosztową procesu generacji głębokich sieci neuronowych.
\end{itemize}

Pierwsza hipoteza porusza kwestię efektywności kosztowej jednostek GPU oferowanych w chmurze. Literatura dostarcza dowodów, że stosowanie tego typu jednostek prowadzi do krótszego czasu konstruowania sieci neuronowych, por. \citet{jermain2015} i \citet{litvinenko2014}. Nie jest jednak oczywistym, czy efektywniejszym kosztowo rozwiązaniem w przypadku architektury chmurowej są jednostki GPU czy CPU. Pomimo niższego czasu potrzebnego na estymację modelu, cena jednostek wyposażonych w GPU może być na tyle wysoka, że ich wynajem nie jest optymalny w sensie kosztowym.

Następnie chcemy skupić się na porównaniu ofert różnych dostawców usług przetwarzania w chmurze. Dokonamy porównania efektywności kosztowej jednostek wyposażonych w procesory graficzne, dostarczanych przez Amazon, Microsoft oraz Google.

Efektem pracy będzie dostarczenie narzędzia przeprowadzającego eksperymenty i pozwalającego na wybór najefektywniejszej kosztowo jednostki obliczeniowej. To narzędzie ma na
celu przeprowadzenie optymalizacji kosztowej w zależności od rozważanych jednostek obliczeniowych i przedstawionego problemu.

Na podstawie prezentowanego badania zostanie wybrany zbiór decyzji niezdominowanych (optymalnych w sensie Pareto) dla rozważanego problemu wielokryterialnego. Eksperyment przeprowadzany będzie poprzez dokonywanie wielokrotnych, empirycznych pomiarów dla każdej rozważanej jednostki obliczeniowej. Wynikiem każdego pomiaru będzie koszt wynajmu serwera oraz czas potrzebny na skonstruowanie modelu. Wybranym do przeprowadzania eksperymentu problemem jest przetwarzanie obrazu (stworzenie klasyfikatora). Następnie przeprowadzimy agregację otrzymanych wyników oraz analizy zbioru dostępnych decyzji, by ostatecznie przedstawić zbiór decyzji optymalnych w sensie Pareto.

\clearpage


\section{Deep learning w chmurze}
\subsection{Sieci neuronowe i deep learning}

Do najpopularniejszych przykładów użycia metod \textit{machine learningowych} zaliczyć można dostosowywanie wyników wyszukiwań internetowych, filtrowanie i personalizacja powiadomień w mediach społecznościowych, rekomendacje produktów i reklama internetowa. Sztuczne sieci neuronowe okazały się jednak szczególnie użyteczne w rozpoznawaniu dźwięku, identyfikowaniu obiektów na obrazach i tłumaczeniu tekstu (\citet{schmidhuber2015} i \citet{goodfellow2016}).

Standardowa architektura sztucznej sieci neuronowej składa się z wielu elementarnych, połączonych jednostek - neuronów, które przyjmując pewien sygnał wejściowy, zwracają przetworzony sygnał wyjściowy. Neurony pierwszej, wejściowej warstwy przyjmują sygnał z danych uczących, podczas gdy neurony kolejnych warstw aktywowane są poprzez sygnały pochodzące z warstw je poprzedzających. Neurony ostatniej, wyjściowej warstwy, zwracają rezultat obliczeń, który następnie może prowadzić do wywołania pewnej akcji. Uczenie sieci neuronowej sprowadza się do znalezienia architektury i wchodzącej w jej skład wag, które sprawią, że będzie ona zachowywała się w sposób pożądany, na przykład poprzez możliwość wykrywania pewnych cech sygnału wejściowego. W zależności od stopnia skomplikowania problemu, przyjętej architektury, hiperparametrów, odnalezienie optymalnego rozwiązania może wymagać długich, skomplikowanych obliczeń wykonywanych na przestrzeni wielu warstw sieci, gdzie każda z nich przekształca i agreguje otrzymywany sygnał wejściowy. W tym rozdziale opisane zostaną modele głębokich sieci neuronowych (\textit{deep neural networks}), ze szczególnym uwzględnieniem architektur wykorzystywanych do rozpoznawania obrazu - historii ich powstania, sposobu działania, wiązanymi z nimi oczekiwaniami i obecnie stosowanymi praktykami.

\begin{figure}[h]
  \centering
\includegraphics[scale=0.5]{../obrazy/fig:NN.png}
\caption{Standardowa architektura sieci neuronowej - składa się z warstwy wejściowej (\textit{input layer}), warstw ukrytych (\textit{hidden layers}) oraz warstwy wyjściowej  (\textit{output layer}).
\label{fig:NN}}
\end{figure}

Wraz z nadejściem elektroniki i komputerów, badacze rozpoczęli podejmować próby replikacji ludzkich procesów myślowych na systemy logiczne. Choć pierwsze kroki w tej dziedzinie zostały podjęte przez \citet{mcculloch1943} oraz \citet{hebb1949}, istotny z punktu widzenia uczenia maszynowego przełom w sztucznych sieciach neuronowych został dokonany w pracy \citet{rosenblatt1957}, gdzie po raz pierwszy wprowadzono koncept perceptronu - algorytmu, który miał pozwolić na poprawne rozpoznawanie obrazów. Perceptron jest jednym z najbardziej podstawowych przykładów sieci neuronowych, jego schemat został przedstawiony na rysunku \ref{fig:perceptron}.

\begin{figure}[h]
  \centering
\includegraphics[scale=0.6]{../obrazy/fig:perceptron.png}
\caption{Perceptron przedstawiony na przykładzie składa się z dwóch warstw – warstwy wejściowej z trzema wartościami wejściowymi (\(x_1, x_2, x_3 \)) oraz warstwy wyjściowej. \label{fig:perceptron}}
\end{figure}

Przyjmuje on wektor \(\mathbf{x}\) binarnych wartości wejściowych \(x_1, \ldots, x_n \). Każda z tych wartości wejściowych jest przyjmowana z wagą \(w_1, \ldots, w_n \), będącą liczbą rzeczywistą. Owe parametry można w intuicyjny sposób przedstawić jako wpływ (mający wartość i kierunek) każdego z sygnałów wejściowych na sygnał wyjściowy. Ostatecznie ważona suma wartości wejściowych porównywana jest z wartoscią progową (\textit{threshold}), będącą kolejnym parametrem perceptronu. W przypadku, gdy ważona suma jest większa od wartości progowej, funkcja aktywacji zwraca wartość wyjściową 1, w przeciwnym wypadku zwraca 0. Równanie \ref{eq:perc_activation} zawiera formalną reprezentację funkcji aktywacji.

\begin{equation} \label{eq:perc_activation}
  output=\begin{cases}
    0& \text{jeśli $\sum_j$ $w_j$ $x_j$ $<$ threshold},\\
    1& \text{jeśli $\sum_j$ $w_j$ $x_j$ $\geq$ threshold}.
  \end{cases}
\end{equation}

Algorytm uczenia perceptronu w $T$ iteracjach dla $N$ obserwacji uczących i $M$ wag przedstawiony został w Algorytmie \ref{alg:perceptron}.

\begin{algorithm}
\caption{Algorytm uczenia perceptronu}\label{alg:perceptron}
  \begin{algorithmic}[1]
    \State Inicjalizacja losowych wag $W$
    \For{$t \in \{1,\dots,T\}$}
      \For{$i \in \{1,\dots,N\}$}
        \State {$E = y - \sigma(W_{t-1} \cdot \mathbf{x_i})$}
        \For{$j \in \{1,\dots,M\}$}
          \State {$W_j = W_j + \alpha \cdot E \cdot \sigma^\prime(W_j \cdot x) \cdot x_j$}
        \EndFor
      \EndFor
    \EndFor
  \end{algorithmic}
\end{algorithm}

\citet{widrow1960} przedstawili ADALINE - jednowarstwową sztuczną sieć neuronową (Rysunek \ref{fig:ADALINE}), która uczona była za pomocą szczególnego przypadku do dzisiaj wykorzystywanej metody optymalizacji opartej na gradiencie - \textit{stochastic gradient descent}.

\begin{figure}[h]
  \centering
\includegraphics[scale=0.5]{../obrazy/fig:ADALINE.png}
\caption{Architektura ADALINE (\textit{Adaptive Linear Neuron}), która bazowała na projekcie sieci zaprezentowanym przez \citet{mcculloch1943}. Istotna różnica polega na dobraniu algorytmu optymalizacji wag. \label{fig:ADALINE}}
\end{figure}

Jednym z pierwszych przykładów architektury, którą można uznać za wielowarstwową sieć neuronową jest zaprojektowany w pracach \citet{ivakhnenko1965}, \citet{ivakhnenko1967} oraz \citet{ivakhnenko1968} system będący reprezentantem rodziny metod GMDH (\textit{Group Method of Data Handling}). Jego charakterystycznymi cechami było wykorzystanie wielomianów Kolmogorova-Gabora jako funkcji aktywacji i użycie zbioru treningowego do uczenia i rozbudowywania kolejnych warstw sieci, by następnie przycinać je z wykorzystaniem zbioru walidacyjnego.

Pomimo obiecujących początków i optymizmu towarzyszącemu nowemu odkryciu, badania nad sztucznymi sieciami neuronowymi zostały w dużej części porzucone za sprawą publikacji \citet{minsky1969}. Nakreślili oni granice możliwości modeli liniowych (w tym perceptronu i jednowarstwowych sieci neuronowych) jako systemu uczącego się. W szczególności udowodnili, że z wykorzystaniem tego typu algorytmów nie da się odwzorować funkcji logicznej XOR. Ich odkrycie zakończyło pierwszy okres popularności sieci neuronowych, zwany cybernetyką.

Kolejna fala popularności sieci neuronowych została zapoczątkowana przez interdyscyplinarny ruch - koneksjonizm (\textit{connectionism}), określany również jako modele PDP (równoległego rozproszonego przetwarzania informacji). Wiele idei wywodzących się z cybernetyki zostało w tym okresie zaadaptowanych do badań nad charakterem poznania, por. \citet{touretzky1985}. Jedną z nich jest pomysł rozproszonej reprezentacji \citep{hinton1986}, opisywany także w pracy \citet{touretzky1985} jako inspirowany naturą i z niej się wywodzący. Przedstawia on założenie, że każdy sygnał wejściowy powinien być reprezentowany przez wiele cech systemu, a każda cecha powinna być włączona w tworzenie reprezentacji możliwie wielu sygnałów wejściowych. Wprowadzenie tej architektury było również odpowiedzią na krytykę koneksjonizmu, a jej wykorzystanie dowodem na to, że teorie wchodzące w skład tego paradygmatu pozwalają na stworzenie systemu zdolnego do rozwiązywania skomplikowanych problemów. Koncept rozproszonej reprezentacji pozostaje kluczowym komponentem konstruowanych obecnie sieci neuronowych, por. \citet{goodfellow2016}.

W tym okresie przedstawiono również inny koncept stosowany dotychczas - jednostki konwolucyjne (\textit{convolutional units}). Celem pracy \citet{fukushima1980} było zaproponowanie sztucznej sieci neuronowej, która mogłaby być wiernym modelem procesu rozpoznawania wzorów i kszałtów geometrycznych przez człowieka. Przedstawiono w niej model uczenia bez nadzoru, nazwany wówczas Neocognitron (będącą rozszerzeniem modelu Cognitron, zaproponowanego wcześniej przez Autora, por. \citet{fukushima1975}) i zastosowano architekturę, która jest protoplastą szeroko stosowanych sieci konwolucyjnych (używanych jednak obecnie głównie do rozwiązywania problemów ,,z nauczycielem''). Fukushima wprowadzał do modelu jako sygnał dwuwymiarową macierz, po której następujował szereg struktur składających się z pary warstw ukrytych odpowiadających komórkom niższego i wyższego rzędu (\textit{S-layer} oraz \textit{C-layer}). Schemat opisywanej architektury zaprezentowany został na Rysunku \ref{fig:neocognitron}. Istotnym z punktu widzenia \textit{deep learningu} i \textit{pattern recognition} było zastosowanie w niej warstw złożonych z jednostek konwolucyjnych, których pole receptywne było dwuwymiarową macierzą skokowo przesuwaną przez macierz wartości wejściowych (Rysunek \ref{fig:convolution}). Zwracane przez warstwy konwolucyjne macierze stanowiły wartości wejściowe dla warstw konwolucyjnych wyższego rzędu, operujących na wyższym poziomie abstrakcji. Każdy z elementów ostatniej warstwy tak skonstruowanej sieci neuronowej odpowiadał tylko na jeden rodzaj sygnału wejściowego. Owe zabiegi pozwoliły na skonstruowanie modelu, który byłby niewrażliwy na małe zmiany w kształcie, obrót lub translację sygnału wejściowego (Rysunek \ref{fig:distortion}). Należy jednak zwrócić uwagę na fakt, że ,,niewrażliwość'' na tego typu przekształcenia nie sprowadza się do takiego samego traktowania przekształcanego sygnału wejściowego przez pierwszą warstwę sieci. Sygnał wejściowy poddany na przykład translacji interpretowany jest przez inny zestaw neuronów niż pierwotny sygnał. Te same cechy tych sygnałów są jednak, ze względu na wykorzystanie jednostek konwolucyjnych, wykrywane niezależnie od ich położenia - mowa zatem raczej o równoważności interpretacji, a nie jej niezmienności w zależności od tego typu zmian w sygnale wejściowym.

\begin{figure}[h]
  \centering
\includegraphics[scale=0.7]{../obrazy/fig:neocognitron.png}
\caption{Architektura modelu Neocognitron. W jej skład wchodzi warstwa wejściowa \(U_0\), po której następują trzy pary warstw ukrytych \(U_{Si} \) i \(U_{Ci} \) - odpowiednio niższego i wyższego rzędu. \label{fig:neocognitron}}
\end{figure}

\begin{figure}[h]
  \centering
\includegraphics[scale=0.5]{../obrazy/fig:convolution.png}
\caption{Ilustracja połączeń dla przykładowych jednostek konwolucyjnych, zaproponowanych przez Fukushima. \label{fig:convolution}}
\end{figure}

\begin{figure}[h]
  \centering
\includegraphics[scale=0.5]{../obrazy/fig:distortion.png}
\caption{Przykłady wartości wejściowych (b - g) będących pewną modyfikacją pierwotnego obrazu (a), dla których Neocognitron zwraca ten sam rezultat (h). \label{fig:distortion}}
\end{figure}

Oprócz przełomu na polu nauk poznawczych, nastąpił również rozwój metod optymalizacyjnych. Praca \citet{rumelhart1986} dowiodła empirycznie skuteczność algorytmu propagacji wstecznej (\textit{backpropagation}) w przyspieszaniu procesu uczenia sieci neuronowych, co przyczyniło się do wzrostu popularności owej metody i pozwoliło na wykorzystywanie sieci neuronowych do szerszego zakresu problemów. Ta procedura optymalizacji, przedstawiona po raz pierwszy w pracy \citet{linnainmaa1970}, polegała na iteracyjnym korygowaniu wartości wag w sieciach neuronowych w celu minimalizacji wybranego kryterium błędu (Algorytm \ref{alg:backpropagation}), por. również \citet{hecht1988}. W pracy \citet{rumelhart1986} neurony warstw ukrytych, których stan lub zadanie nie zostało zawczasu ustalone, mogły reprezentować istotne cechy sygnału wejściowego. Możliwość wykrywania cech sygnału wejściowego bez ich wskazywania w sposób jawny, miała w znaczny sposób odróżniać przedstawiony system od sposobów uczenia skonstruowanych w erze cybernetyki. Potwierdzeniem tej tezy był fakt wygrania międzynarodowego konkursu predykcyjnego poprzez wykorzystanie modelu sieci neuronowych uczonego z użyciem algorytmu propagacji wstecznej \citep{wan1993}. W publikacji \citet{rumelhart1986} zaproponowano również pewną modyfikację do metody SGD - metodę pędu (\textit{momentum method}), która, inspirowana zjawiskiem fizycznym, miała zapobiec zbyt długiemu oscylowaniu wokół ekstremów funkcji celu w optymalizacji gradientowej.

\begin{algorithm}
\caption{Algorytm propagacji wstecznej}\label{alg:backpropagation}
  \begin{algorithmic}[1]
    \State Inicjalizacja losowych wag $W$
    \For{$t \in \{1,\dots,T\}$}
      \For{$e \in \{1,\dots,N\}$}
        \State Oblicz wartości wejściowe $in_i$ i wyjściowe $out_i$ każdego neuronu dla $x_i$
        \For{$(j,i)$ (wszystkich wag idąc wstecz)}
          \If{i jest neuronem warstwy wyjściowej}
            \State $ \Delta_i = (y_i-out_i) \cdot \sigma^\prime(in_i) $
          \Else
            \State $ \Delta_i = \sigma^\prime(in_i)\sum_{k}w_{i,k}\Delta_k $
          \EndIf
          \State $ W_{j,i} = W_{j,i}+\alpha \cdot {out_j} \cdot \Delta_i $
        \EndFor
      \EndFor
    \EndFor
  \end{algorithmic}
\end{algorithm}

Algorytm propagacji wstecznej, choć będący bardzo użytecznym narzędziem, nie spełnił pokładanych w nim nadziei i nie był rozwiazaniem wszystkich problemów tego typu modeli. Pomimo faktu, że w teorii pozwalał na uczenie głebokich sieci, wydawało się, że spektrum jego zastosowań mieści się tylko w zakresie płytkich architektur. Większość sieci, które były projektowane ograniczały się do kilku warstw ukrytych, jako że dodawanie kolejnych warstw nie przynosiło oczekiwanych rezultatów. W kolejnych latach zaproponowano wiele usprawnień w zakresie algorytmów optymalizujących proces uczenia. Oprócz uprzednio wspomnianej metody pędu, przedstawiono również metody polegające na ustaleniu osobnych szybkości uczenia (\textit{learning rate}) dla różnych sieci, por. \citet{jacobs1989}, a także \citet{almeida1990}. Uzasadnieniem tego zabiegu jest różnica w szybkości uczenia pomiędzy różnymi warstwami sieci, ale także pozwala on na dynamiczne zmniejszanie \textit{learning rate} w przypadku wag, w których następują częste zmiany kierunku gradientu. \citet{jacobs1989} zaproponował również połączenie tej metody z metodą pędu.
Osobnym wariantem propagacji wstecznej jest Rprop - metoda zaproponowana przez \citet{riedmiller1993}, jak również metody od niej pochodzące: iRprop+ \citep{igel2003} oraz RMSProp \citep{tieleman2012}. Istotą Rprop jest wykorzystywanie wyłącznie znaku gradientu przy estymowaniu modelu z wykorzystaniem całego zbioru danych (\textit{full-batch learning}) w celu zminimalizowania trudności z wyborem globalnej \textit{learning rate}. Krok uczenia jest zwiększany w przypadku, gdy znak gradientu pozostaje niezmienny, w przeciwnym wypadku zostaje zmniejszany. Optymalizacja z wykorzystaniem części zbioru danych (\textit{mini-batch learning}) opiera się na założeniu o uśrednieniu kierunku gradientu dla wielu partii zbioru danych. Założenie to nie jest spełnione w przypadku wykorzystania algorytmu Rprop i z tego powodu nie jest wskazane stosowanie tej metody w połączeniu z \textit{mini-batch learning}. Ulepszenie algorytmu Rprop - iRProp+ polega na sprawdzeniu dodatkowych warunków dotyczących zmiany funkcji błędu poza sprawdzeniem zmiany kierunku gradientu. W przypadku zmiany kierunku gradientu i poprawy funkcji celu (zmniejszenia błędu), nie następuje korekta wag. Z kolei rozwiązaniem problemu braku możliwości uczenia modelu z wykorzystaniem wielu partii zbioru danych jest wspomniana metoda RMSProp. Metodę Rprop można alternatywnie przedstawić jako wykorzystanie klasycznej metody gradientowej przy dodatkowym dzieleniu przez wartość gradientu w każdym kolejnym kroku.
Istotą RMSProp jest wprowadzenie ruchomej średniej kwadratu gradientu dla każdej z wag, tak, by w każdej kolejnej partii zbioru danych zamiast dzielenia przez wartość gradientu, dzielić przez jej pierwiastek kwadratowy (Równanie \ref{eq:rmsprop_ma}).

\begin{equation} \label{eq:rmsprop_ma}
  {MA}(w,t) = 0,9 \cdot {MA}(w, t-1) + 0,1 \cdot (\frac{\partial E}{\partial w}(t))^2
\end{equation}

Ówczesny obszar badań obejmował również prace nad mniej skomplikowanymi modelami sieci neuronowych, które miałyby wysoką zdolność uogólniania. Próbowano w ten sposób zapobiec przeuczaniu tego typu modeli, które ze względu na ich pojemność informacyjną jest dużym ryzykiem. Przykładem popularnego podejścia, które skupiało się na znalezieniu odpowiedniego balansu między wariancją, a obciążeniem modelu (\textit{bias-variance dillemma}) jest metoda \textit{weight decay} \citep{hanson1990}. Sprowadza się ona do ograniczania wartości wag za pomocą metod regularyzacji, w tym \textit{L1} lub \textit{L2}, podobnie jak ma to miejsce w modelach regresji. Ponieważ propagacja wsteczna jest szczególnym przypadkiem wielowymiarowych, nieliniowych metod regresyjnych, a wartości wejściowe są często skorelowane, nie są spełnione warunki potrzebne do uzyskania nieobciążonego estymatora o najmniejszej wariancji. Wtedy sztuczne obciążenie estymatora może zmniejszyć jego wariancję, poprawiając stabilność modelu.

Alernatywną metodą do osiągnięcia tego samego celu jest wykorzystanie podejścia bayesowskiego, por. \citet{nowlan1992}, \citet{hinton1993}, a także \citet{edwards1993}. Bayesowska interpretacja \textit{weight decay} polega na minimalizowaniu logarytmów prawdopodobieństwa - logarytmu prawdopodobieństwa a posteriori ($P(W|D)$) i logarytmu prawdopodobieństwa a priori ($P(W)$), które odpowiadają kosztowi (na przykład kwadratowej funkcji kosztu) i wyrażeniu regularyzującemu. Wartości wag \textit{a priori} mogą również zostać \textit{implicite} zawarte w dodatkowych karach \citep{mackay1992} lub poprzez użycie metod opartych na zbiorze walidacyjnym \citep{hastie1990}, kryterium informacyjnym Akaike \citep{akaike1974} lub oczekiwanym błędzie predykcji \citep{moody1994}. Do podobnych sposobów poprawy zdolności uogólniania sieci neuronowych zaliczyć można oparte na upraszczaniu architekury, jak sekwencyjne budowanie sieci warstwa po warstwie \citep{ivakhnenko1968}, przycinanie wartości wejściowych \citep{moody1991}, a także przycinanie wag \citep{lecun1989a}.

W 1989 roku stworzono sieć neuronową w oparciu o przedstawiony wcześniej model Neocognitron \citet{fukushima1980} i z wykorzystaniem algorytmu propagacji wstecznej, por. \citet{lecun1989b}. Do skonstruowania tego systemu użytu również jednostek konwolucyjnych, idei współdzielenia wag i zasilono go nowym konceptem - \textit{max pooling}. Współdzielenie wag (\textit{weight sharing}) opisane zostało w pracy \citet{rumelhart1986} dla problemu T-C i polegało na sterowaniu wieloma połączeniami jednym paramentrem wagi. W ten sposób nie tylko znacznie redukuje się liczbę potrzebnych parametrów w sieci, ale także opisuje się topologię przedstawionego zadania. Architektura omawianej sieci została przedstawiona na Rysunku \ref{fig:lecunmnist}. Składa się z trzech warstw ukrytych (H1, H2, H3). Połączenia pomiędzy warstwą H1 a H2 zostały sztucznie ograniczone. Warstwa H1 składa się z 12 grup 64 jednostek, gdzie każda z nich jest niezależnym zbiorem map cech o wymiarze 8x8. Każda jednostka w pierwszej warstwie przyjmuje wartość wejściową w postaci macierzy 5x5 ze źródłowego obrazu. Ponieważ z dużą pewnością można stwierdzić, że cechy istotne w jednej części obrazu będą również istotne w innych częściach obrazu, zastosowano w tej warstwie technikę \textit{weight sharing}. Zatem każda jednostka ma 26 wag, gdzie 25 jest współdzielone z resztą grupy, a 26, \textit{bias}, jest dla niej unikalna. W efekcie warstwa H1 składa się z $8\cdot8\cdot12=768$ jednostek, $768\cdot26=19968$ połączeń, ale jedynie $768+26\cdot25=1068$  parametrów. Podobna logika zastosowana została w warstwie H2, podczas gdy warstwa H3 składa się z 30 jednostek. w pełni połączonych z warstwą H2. Uczenie przyspieszono za pomocą procesorów graficznych, które okazały się istotnym komponentem w tym podejściu. Ponadto, publikacja ta wprowadziła również zbiór danych \textit{MNIST}, który do dzisiaj jest wykorzystywany jako poziom porównawczy dla wielu architektur sieci neuronowych. W przeciwieństwie do wielu poprzednich eksperymentów, w tej pracy wartością wejściową do modelu były surowe obrazy zamiast przygotowanych wektorów, co miało zademonstrować możliwość radzenia sobie z nieprzetworzonymi danymi przez tego typu systemy.

\begin{figure}[h]
  \centering
\includegraphics[scale=0.6]{../obrazy/fig:lecunmnist.png}
\caption{Model wykorzystany przez \citet{lecun1989b} do problemu identyfikacji kodów pocztowych, który jest jednym z pierwszych przykładów wykorzystania konwolucyjnych sieci neuronowych na szeroką skalę. \label{fig:lecunmnist}}
\end{figure}

Istotne spostrzeżenie dotyczące sieci neuronowych zostało przedstawione w pracy \citet{hochreiter1991}. Wyjaśnił, dlaczego głębokie sieci neuronowe są trudne do uczenia z wykorzystaniem propagacji wstecznej (na co wcześniej wskazywały wyniki eksperymentów). Znaleziona przez niego przyczyna określona została później problemem znikającego i eksplodującego gradientu (\textit{vanishing gradients, exploding gradients}). W przypadku standardowych funkcji aktywacji, propagowany wstecz błąd w tempie wykładniczo zależnym od liczby warstw maleje bądź rośnie. Z biegiem czasu przedstawiono liczne podejścia do radzenia sobie z tym problemem, takie jak wstępne uczenie bez nadzoru (i w efekcie kompresja sygnału wejściowego), wykorzystanie procesorów graficznych, optymalizację bez wykorzystania Hesjanów lub całkowite uniknięcie stosowania metod optymalizacji opartych na gradiencie.

Prace nad Neocognitronem \citep{fukushima1980}, jak również wcześniejsze prace \citep{ivakhnenko1968} były inspiracją do stworzenia innego systemu uczącego się - Cresceptron, por. \citet{weng1992}, który dostosowuje swoją architekturę podczas uczenia. Cresceptron wykorzystuje technikę zwaną \textit{max pooling}, gdzie w jednostce macierz wejściowa zostaje zastąpiona przez jej maksymalną wartość. (\textit{Max pooling} oraz \textit{average pooling} zostały zaprezentowane zostały na rysunku \ref{fig:pooling}). Konwolucyjne sieci neuronowe, które wykorzystują tę technikę i uczone są za pomocą propagacji wstecznej zostały popularnym wyborem w późniejszych zastosowaniach i były częścią wielu kolejnych systemów uczących się.

\begin{figure}[h]
  \centering
\includegraphics[scale=0.2]{../obrazy/fig:pooling.png}
\caption{Macierze wejściowa partycjonowana jest na skończoną liczbę mniejszych macierzy. \textit{Average pooling} przekształca każdą z tych macierzy w liczbę będącą średnią jej wartościa, podczas gdy \textit{max pooling} przekształca je w liczbę będącą maksymalną jej wartością. \label{fig:pooling}}
\end{figure}

Ruch koneksjonizmu utrzymywał zainteresowanie środowisk naukowych i biznesowych tylko do połowy lat 90. Nieproporcjonalne oczekiwania co do możliwości sztucznych sieci neuronowych oraz pojawienie się innych, bardziej obiecujących metod (jak maszyny wektorów nośnych) sprawiły, że inwestorzy wycofali swoje wsparcie. Na przełomie wieków istotne publikacje zostały przedstawione na polu rekurencyjnych sieci neuronowych, por. \citet{schmidhuber1997}, \citet{schuster1997} i \citet{graves2005}.

Aktualne zainteresowanie głębokimi sieciami neuronowymi zostało zapoczątkowane jako skutek badań nad modelami o skomplikowanej architekturze, por. \citet{hinton2006a}, \citet{hinton2006b}, a także \citet{bengio2007a}. Wtedy też tego typu architektury sieci neuronowych otrzymały określenie \textit{głębokich}. Miało na celu podkreślenie faktu wykorzystywania wielu ukrytych warstw i wstępnego trenowania sieci z wykorzystaniem technik uczenia bez nadzoru, co dotychczas nie było często eksplorowanym kierunkiem badań z powodu niedostatecznej mocy obliczeniowej. Istotność tych cech w sieciach neuronowych jest podkreślana w pracach \citet{bengio2007b} i \citet{bengio2011}.
W tym okresie przedstawiono również \textit{deep belief networks}, sieci neuronowe będące stosem \textit{Restricted Boltzmann Machines}, które z kolei zostały zaprezentowane w pracy \citet{smolensky1986} i są pewnym rozwinięciem klasycznych \textit{Boltzmann Machines}, por. \citet{hinton1986}. Algorytmy te znalazły od tego czasu zastosowanie w zagadnieniach takich jak redukcja wymiarów, klasyfikacja, silniki rekomendacyjne, \textit{feature learning} i \textit{topic modelling}. Istotą działania tych architektur jest pomysł, by każda kolejna warstwa przyjmowała reprezentację wzorów i cech z wcześniejszej warstwy i uczyła się ją przetwarzać bez nadzoru. Również w 2006 roku dokonana została istotna poprawa wyniku na omówionym zbiorze MNIST przez \citet{ranzato2006}, którzy osiągnęli błąd predykcji na poziomie 0,39\% bez wykorzystania uczenia wstępnego bez nadzoru. W tym okresie rozpoczęto korzystanie z procesorów graficznych w celu przyspieszenia uczenia zarówno sieci neuronowych \citep{oh2004}, jak i sieci konwolucyjnych \citep{chellapilla2006} - osiągnięto odpowiednio około 20 razy lepsze i około 4 razy lepsze czasy w porównaniu do wykorzystania wyłącznie jednostek centralnych.
W 2007 roku, propagacja wsteczna została po raz pierwszy wykorzystana do uczenia opartej na Neocognitronie i Cresceptronie sieci neuronowej, por. \citet{ranzato2007}. Konwolucyjne sieci neuronowe, wykorzystujące max-pooling i propagację wsteczną okazały się niezbędną częścią późniejszych architektur, które wygrywały międzynarodowe konkursy w rozpoznawaniu obrazu. Jednym z pierwszych sukcesów sieci konwolucyjnych na arenie światowej było wykorzystanie ich jako części systemu, który miał wyróżniać istotne regiony w nagraniach z kamer przemysłowych (\citet{jain2009} oraz \citet{yang2009}).

Ważnym sygnałem o rosnącej roli procesorów graficznych okazała się praca \citet{ciresan2010}. Model w niej przedstawiony nie zawierał żadnych przełomowych pomysłów z punktu widzenia architektury (co więcej, nie korzystał z uczenia wstępnego, ani jednostek konwolucyjnych), był jednak uczony z wykorzystaniem implementacji propagacji wstecznej na procesorach graficznych, która pozwoliła przyspieszyć cały proces 50-krotnie. Rok później, w publikacji \citet{ciresan2011a} zaprezentowano implementację uczenia sieci konwolucyjnych z max-poolingiem z wykorzystaniem GPU. Wtedy także dokonano przełomowego osiągnięcia w zakresie rozpoznawania obrazu, pokonując ludzkie zdolności poznawcze w tym zakresie podczas konkursu w rozpoznawaniu znaków drogowych, por. \citet{ciresan2011b}. Osiągnięty błąd predykcji 0,56\% okazał się dwukrotnie lepszy od błędu ludzkiego, trzykrotnie lepszy od kolejnej sieci neuronowej i sześciokrotnie lepszy od systemu nie opartego na sieciach neuronowych. Podobnego rodzaju architektura osiągnęła również najlepszy do tego czasu wynik w klasyfikacji obrazów ze zbioru danych MNIST
(0,2\%), co również było lepszym rezultatem od błędu predykcji przeciętnego człowieka.
Kolejnym osiągnięciem było uzyskanie pierwszego miejsca w problemie klasyfikacji na zbiorze danych ImageNet przez architekturę zaproponowaną przez \citet{krizhevsky2012}, bazującą na przedstawionych wcześniej modelach konwolucyjnych (w przypadku tego zbioru danych wartością wejściową były obrazy o rozmiarach 256x256 pikseli, w przeciwieństwie do znacznie mniejszych obrazów znaków drogowych - 48x48 pikseli).

Głębokie sieci neuronowe osiagały bardzo obiecujące wyniki w problemach klasyfikacji obrazów, jednak prawdopodobnie największe nadzieje są wiązane z rozpoznawaniem obiektów na obrazie, co mogłoby okazać się bardzo przydatne w intepretowaniu rezultatów badań medycznych jak zdjęcia wykonywane w celu diagnozowania chorób nowotworowych. Systemem, który wygrał konkurs dotyczący tych zagadnień (ICPR 2012 Contest on Mitosis Detection in Breast Cancer Historical Images, 2012) była sieć neuronowa stworzona przez \citet{ciresan2013}. Innym rodzajem problemu, który skutecznie można rozwiazań z wykorzystaniem tego typu modeli jest zagadnienie segmentacji pikseli na obrazach \citep{ciresan2012}.

Sztuczne sieci neuronowe znalazły dotychczas zastosowanie w problemach uczeniach z nadzorem, bez nadzoru, jak również w \textit{reinforcement learning}. Najważniejszymi kwestiami do zaadresowania w najbliższej przyszłości są te związane z podejściem do ich konstruowania. Należy odpowiedzieć na pytanie, jak sprawić, by systemy o wciąż wąskich obszarach specjalizacji, były zdolne do większego uogólniania. W przypadku rozpoznawania elementów na obrazach należy wzorować się na procesie poznawczym człowieka, który aktywnie wyszukuje wzorów na obrazie i zwraca uwagę na istotne kwestie. W tym wypadku warto również mieć na uwadze takie aspekty, jak samo etykietowanie danych, które jest procesem silnie humanocentrycznym, co poruszone zostało także w pracy \citet{misra2016}.

\clearpage

\subsection{Cloud computing}

\noindent
W tym rozdziale omówiony zostanie paradygmat przetwarzania danych w chmurze. Przedstawione zostaną jego kluczowe cechy, wymagania stawiane dostawcom i definicje często spotykanych pojęć. Następnie opiszemy najpopularniejsze modele świadczonych usług, modele implementacji i rozwiązania technologiczne leżące u podstaw opisywanego paradygmatu, by finalnie przejść do aspektów ekonomicznych i przybliżenia rynku \textit{cloud computingu} wraz z opisem największych dostawców.

\subsubsection{Definicje, kluczowe charakterystyki}

\noindent
Przetwarzanie danych w chmurze definiowane jest w pracach \citet{buyya2009} oraz \citet{calheiros2010} jako ,,rodzaj równoległego i rozproszonego systemu, składającego się z licznych, połączonych ze sobą i wirtualizowanych serwerów, która są dynamicznie przydzielane na podstawie ustaleń pomiędzy dostawcą a konsumentami, dokonanych za pośrednictwem dedykowanego serwisu''. Innymi słowy, w tym modelu pamięć, moc obliczeniowa znajdują się ,,w chmurze'', która jest zbiorem centrów danych (\textit{data centers}), posiadanych oraz utrzymywanych przez zewnętrzny podmiot. Konsumenci otrzymują dostęp do owej infrastruktury lub serwisów na niej bazujących przez wystosowanie żądania odwzorowującego ich aktualne potrzeby na tego typu usługi, bez wymagania rezerwacji z dużym wyprzedzeniem czasowym i znacznego nakładu kapitału (\textit{pay-as-you go model}).

To podejście, odnoszące się do świadczenia usług w zakresie dostarczania infrastruktury, mocy obliczeniowej lub aplikacji za pośrednictwem Internetu, stało się jednym z głównych paradygmatów w tej dziedzinie nauki i biznesu. Efektem tego optymizmu jest wizja dostarczania mocy obliczeniowej jako kolejnego rodzaju powszechnie dostępnego medium. Ma ono zaspokajać, ze stosunkowo niewielkim opóźnieniem, rosnące w erze digitalizacji zapotrzebowanie na jednostki będące w stanie wykonywać skomplikowane obliczenia. Leonard Kleinrock, naukowiec, który wniósł istotny wkład w budowę Internetu, twierdził w 1969 jako główny inżynier projektu ARPANET, że: ,,W chwili obecnej, sieci komputerowe są wciąż we wczesnym okresie rozwoju, jednakże wraz z ich dojrzewaniem obserwować będziemy rozpowszechnienie narzędzi komputerowych (\textit{computer utilities}), które jak obecnie elektryczność lub sieci telefoniczne, będą służyć gospodarstwom domowym i przedsiębiorstwom w całym kraju''  \citep{kleinrock2005}. Kleinrock skutecznie przewidział nie tylko mającą nadejść powszechność połączenia internetowego, lecz również możliwość wynajmowania maszyn wirtualnych w sposób zdalny, z natychmiastowym skutkiem, w zależności od aktualnego popytu - tak jak ma to miejsce przypadku każdego innego rodzaju powszechnie dostępnego medium. Również Nicholas Carr porównuje potencjalne konsekwencje nadchodzącej zmiany do tworzenia sieci elektrycznej na początku XX w., por. \citet{carr2008}. Kontynuując tę analogię, opłaty za tego typu usługę naliczane są tylko w wypadku, gdy konsument korzystał z dostępnych mu zasobów. Ponadto, podejście to eliminiuje konieczność utrzymywania własnej infrastruktury informatycznej - rewolucja w szczególności dotyczy sposobu projektowania rozwiązań technologicznych w przedsiębiorstwach, które dotychczas w tej materii opierały się na założeniu pełnej lub częściowej samodzielności centrów danych. Istotność tego typu serwisów podkreśla w swojej pracy również Armburst \citep{armburst2010} określając je jako długo wyczekiwane marzenie dostarczenia przetwarzania danych jako szeroko dostępnego medium, które będzie napędzać transformację przemysłu technologii informacyjnych, czyniąc jednocześnie udostępnianie oprogramowania coraz atrakcyjniejszym rodzajem świadczonych usług.

Oprócz korzyści strukturalnych i biznesowych, ekonomia skali towarzysząca tego typu przedsiewzięciom wpływa na zmniejszenie śladu ekologicznego centrów danych, por. \citet{oecd2014}. Odbywa się to zarówno poprzez zmniejszanie zjawiska określanego jako nadmiernie zaopatrywanie się w zasoby (\textit{over-provisioning}), jak również implementowanie efektywniejszych sposobów przetwarzania danych i tworzenia dedykowanych rozwiązań architektonicznych i instalacyjnych, takich jak systemy chłodzenia. Należy jednak również zwrócić szczególną uwagę na potencjalne zagrożenia związane z wzrastającą popularnością tego typu technologii takich jak bezpieczeństwa i prywatności danych. Zostaną one omówione w późniejszej części rozdziału.

Aby wprowadzić pewne terminy i usystematyzować zbiór pojęć wykorzystywany w niniejszej pracy, oparto się na zestawie rekomendacji wydanym przez Państwowy Instytut Norm i Technologii Stanów Zjednoczonych \citep{mell2011}. Określa on \textit{cloud computing} jako model stworzony w celu umożliwienia wszechobecnego, wygodnego, dostępnego na żądanie, współdzielonego zbioru konfigurowalnych zasobów komputerowych, które mogą zostać natychmiastowo dostarczone przy niewielkim wysiłku zarządzającego i minimalnym stopniu interakcji. Infrastruktura rozwiązań pozwalających na przetwarzanie danych w chmurze jest zestawieniem sprzętu komputerowego oraz oprogramowania, które można opisać jako warstwę fizyczną (sprzęt komputerowy: serwery, dyski twarde i komponenty sieciowe) i warstwę abstrakcyjną (oprogramowanie dostępne na warstwie fizycznej). Model ten cechować musi pięć kluczowych charakterystyk, wyróżnione również są trzy modele świadczonych usług i cztery modele implementacji.

\noindent
Kluczowe charakterystyki:
\begin{itemize}
\item samoobsługowość oraz dostępność na żądanie (\textit{on-demand self-service}) -- klient może jednostronnie wynajmować jednostki obliczeniowe, również w sposób zautomatyzowany,
\item szerokopasmowy dostęp sieciowy (\textit{broad network access}) -- oferowane usługi dostępne są za pośrednictwem sieci, dostęp do nich można uzyskać przez standardowe mechanizmy, z wykorzystaniem zróżnicowanych platform (takich jak laptopy, komputery osobiste, telefony komórkowe czy tablety),
\item błyskawiczna elastyczność (\textit{rapid elasticity}) -- zasoby mogą być w sposób elastyczny rezerwowane i zwalniane, również automatycznie. Z perspektywy konsumenta oferowane zasoby wydają się być nieograniczone,
\item mierzalność usług (\textit{measured service}) -- systemy w sposób automatyczny kontrolują i optymalizują wykorzystanie zasobów dostosowując się do pewnej, uprzednio ustalonej miary (typowo stawki określanej jako płać i korzystaj (\textit{pay-per-use}). Jednocześnie musi być zapewniona pełna transparentność zarówno dla dostawcy usług, jak i konsumenta.
\end{itemize}

\noindent
Modele świadczonych usług :
\begin{itemize}
\item Oprogramowanie jako usługa (\textit{Software as a Service, SaaS}) -- konsument otrzymuje gotowe oprogramowanie, aplikacje, działające w infrastrukturze chmurowej. Do tego typu rozwiązań włącza się różnego rodzaju aplikacje stworzone na potrzeby konkretnych procesów i celów biznesowych. Są one dostępne z różnych rodzajów klientów i urządzeń. Jako przykład tego typu usług zaliczyć można zarówno aplikacje e-mailowe skierowane do odbiorców biznesowych, jak również zintegrowane systemy zarządzania relacjami z klientem. Konsument nie zarządza leżącą u podstaw aplikacji infrastrukturą (w tym siecią, serwerami, systemami operacyjnymi, pamięcią),
\item Platforma jako usługa (\textit{Platform as a Service, PaaS}) -- konsument otrzymuje możliwość wdrażania do infrastruktury chmurowej aplikacji stworzonych z wykorzystaniem wspieranych przez dostawcę języków programowania, bibliotek i narzędzi. Dostawcy tego typu usług posługują się i udostępniają dedykowane interfejsy programistyczne aplikacji (\textit{Application Programming Interface (API)}). Również w tym wypadku konsument nie zarządza infrastrukturą technologiczną,
\item Infrastruktura jako usługa (\textit{Infrastructure as a Service, IaaS}) -- konsument otrzymuje możliwość zarządzania sieciami, pamięcią, jednostkami obliczeniowymi i innymi fundamentalnymi zasobami, mogąc jednocześnie wdrażać i uruchamiać dowolne oprogramowanie, w tym rówież systemy operacyjne i aplikacje. Pozwala to konsumentowi na dużą elastyczność w zaspokajaniu swoich potrzeb informatycznych, nie ma jednak wciąż dostępu do fizycznej infrastruktury.
\end{itemize}

Schemat modeli usług świadczonych w chmurze wraz z porównaniem do tradycyjnych systemów IT zaprezentowany został również na Rysunku \ref{fig:cloudarch}.

\begin{figure}[h]
  \centering
\includegraphics[width=\textwidth, keepaspectratio]{../obrazy/fig:cloudarch.png}
\caption{Stopień ingerencji w oferowane zasoby jest największy w przypadku platformy IaaS, podczas gdy rozwiązania SaaS pozwala użytkownikowi na minimalną inwestycję czasową w zarządzanie infrastrukturą \label{fig:cloudarch}}
\end{figure}

Należy jednak mieć na uwadze, że rozgraniczenie pomiędzy modelem \textit{IaaS} i \textit{PaaS} nie jest aż tak wyraźne jak w przypadku modelu \textit{SaaS}. W niektórych publikacjach \citep{armburst2010} są one traktowane jako grupa rozwiązań, które podzielają więcej podobieństw niż różnic.

\noindent
Modele implementacji:
\begin{itemize}
\item Prywatna chmura (\textit{Private Cloud}) -- infrastruktura zostaje udostępniona na wyłączność dla pojedynczego podmiotu lub organizacji. Może być posiadana i zarządzana przez tę organizację, podmiot trzeci lub dowolną ich kombinację. Może istnieć na terenie dostawcy usług lub poza nim,
\item Zbiorowa chmura (\textit{Community cloud}) -- infrastruktura udostępniona zostaje na wyłączność dla pewnej zbiorowości podmiotów, która mają wspólne cele. Może być posiadana i zarządzana przez jedną lub wiecej z organizacji należących do owej zbiorowości, podmiotu trzeciego lub dowolnej ich kombinacji. Może istnieć na terenie dostawcy usług lub poza nim,
\item Publiczna chmura (\textit{Public cloud}) -- infrastruktura udostępniona dla wszystkich użytkowników. Może być posiadana i zarządzana przez przedsiebiorstwo prywatne lub publiczne, placówkę akademicką lub dowolną ich kombinację. Musi istnieć na terenie dostawcy usług,
\item Hybrydowa chmura (\textit{Hybrid cloud}) -- infrastruktura jest połączeniem dowolnych dwóch lub więcej infrastruktur chmurowych, które pozostają odrębnymi bytami, lecz są związane rozwiązaniami technologicznymi. W przypadku połączenia prywatnej i publicznej chmury pozwala to na utrzymywanie dedykowanej infrastruktury, stanowiącej rdzeń systemu informatycznego przedsiębiorstwa, która w razie wyjątkowego obciążenia systemu lub awarii może zostać wspomagana zasobami pochodzącymi z chmury publicznej, por. \citet{antonopoulos2010}.
\end{itemize}

Ponieważ wiele prywatnych centrów danych korzysta z terminu ,,przetwarzania w chmurze'' w celu opisania swojego modelu biznesowego i przejaskrawienia potencjalnych korzyści, \citet{armburst2010} silnie podkreślają różnicę pomiędzy stosowanymi przez takich dostawców rozwiązaniami, a infrastrukturą posiadającą wszystkie wskazane cechy. Uargumentowane jest to faktem, że ekonomia skali i możliwość błyskawicznego dostosowywania się do zmian w popycie jest w przypadku tego przedsięwzięcia kluczowym czynnikiem, fundamentalnym dla jego opłacalności. Małej i średniej wielkości centra danych (nie posiadające setek tysięcy-milionów serwerów) nie mogą oferować swoim klientom atrakcyjniejszych cen, ponieważ czerpią tylko z podzbioru wymienionych wcześniej możliwości. Ponadto, jak twierdzi \defcitealias{mustafa2015}{Mustafa (2015)}\citetalias{mustafa2015}, paradygmat \textit{cloud computingu} jest w swojej naturze skierowany na potrzeby rynku, podczas gdy klasyczna architektura jest systemocentryczna i nie daje dostawcy bodźców do traktowania wszystkich zgłoszeń z równą wagą. Z wymienionych powodów prywatny model chmury często nie jest przywoływany w debatach nad tego typu rozwiązaniami.

Na tym etapie warto również przywołać pokrewne technologie, takie jak \textit{grid computing} oraz \textit{utility computing}, por. \citet{zhang2010}. \textit{Grid computing}, zwany również przetwarzaniem sieciowym jest paradygmatem rozproszonego przetwarzania danych, który koordynuje zasoby znajdujące się w jednej sieci, by osiągnąć jeden ustalony cel w przetwarzaniu. To rozwiązanie ma swoje korzenie w środowisku naukowym, gdzie wykonywane obliczenia potrzebowały dużej ilości zasobów. Różnica pomiędzy nim a przetwarzaniem w chmurze polega na stopniu wykorzystywania technologii wirtualizacji na wielu poziomach (sprzętowym i aplikacyjnym). Druga z wymienionych technologii, \textit{utility computing}, reprezentuje model dostarczania zasobów na żądanie i fakturowaniu klientów na podstawie ich wykorzystania zamiast stałej stawki. Przetwarzanie w chmurze może być rozpatrywane jako pewnego rodzaju realizacja \textit{utility computingu}. Adaptuje ona schemat ustalania stawek ze względów czysto ekonomicznych.

\subsubsection{Wirtualizacja i zarządzanie zasobami}

\noindent
U podstaw przedstawianej infrastruktury leżą dwie technologie: wirtualizacja oraz zarządzanie zasobami \citep{mustafa2015}. Techniki wirtualizacyjne wykorzystywane są w celu zapewnienia elastycznego i dynamicznego przydzielania mocy obliczeniowej, podczas gdy proces zarządzanie zasobami odpowiada za ich dostarczanie i nadzór nad nimi. Dzieje się to poprzez dzielenie jednej fizycznej maszyny na wiele logicznych systemów operacyjnych z wykorzystaniem hipernadzorcy (\textit{hypervisor}), który emuluje leżący u podstaw model sprzętowy i odpowiada za inicjalizację i nadzorowanie pracy wielu systemów operacyjnych. Rysunek \ref{fig:vm}. przedstawia działanie takiego typu architektury i porównuje go do tradycyjnego podejścia. Wirtualizowany serwer powszechnie nazywany jest maszyną wirtualną (\textit{Virtual Machine, VM}). Cykl życia maszyny wirtualnej obejmuje sześć faz: tworzenie, zawieszenie, wznowienie, zapisanie, migrację oraz niszczenie.

Wirtualizacja po raz pierwszy została wykorzystana na wielką skalę przez IBM w latach 1960 przy systemach klas komputerów określanych jako komputery głównego szeregu (\textit{mainframe systems}). Środowiska wirtualne i systemy operacyjne używane na tych komputerach pozwalały na uruchamianie wielu aplikacji i procesów dla wielu użytkowników jednocześnie i utrzymanie wysokiego poziomu automatyzacji przydzielania zadań. Robert P. Goldberg w 1974 roku opisał potrzebę wykorzystywania maszyn wirtualnych w następujący sposób: ,,Systemy maszyn wirtualnych były oryginalnie zbudowane by naprawić pewne wady architektur trzeciej generacji i wieloprogramowych systemów operacyjnych (\textit{multiprogramming operating systems}, takich jak OS/360'' \citep{goldberg1974}. W latach 1980 i 1990 dominującym podejściem do przetwarzania danych były systemy rozproszeone, aplikacje klient-serwer oraz tanie serwery x86. Osiagnięcia technologiczne ostatnich lat doprowadziły jednak do znacznego wzrostu wydajności stosowania tego typu rozwiązań i wykorzystania wielordzeniowych procesorów na szeroką skalę. Opracowane niedawno, popularne architektury sprzętowe, takie jak rodzina architektur procesorów Intel X86 czy AMD-V silnie czerpią z tego modelu (przykładem mogą również być takie implementacje jak VMware, Xen czy KVM, por. \citet{shroff2010}). Powrót do tego podejścia argumentowany jest także oszczędnością energii \citep{ward2013}, znacznie większą stabilnością tak zaprojektowanego systemu, a w przypadku przedsiębiorstw oferujących usługi przetwarzania w chmurze również możliwością dostosowywania się do błyskawicznie zmieniających się potrzeb konsumentów. Przykładowo w przypadku awarii jednej z maszyn, wszystkie wykonywane przez nią zadania automatycznie zostają migrowane na jedną lub wiele pozostałych jednostek. Ponadto, dostawcy muszą mieć pewność, że są wystarczająco elastyczni, by w czasie rzeczywistym zaspokajać popyt na różne rodzaje jednostek, izolując jednocześnie w wymaganym stopniu użytkowników od fizycznej infrastruktury. To podejście do zarządzania zasobami, ich stałe monitorowanie i możliwość automatycznego reagowania na awaryjne sytuacje muszą być zapewnione w przypadku środowiska \textit{cloud computingowego} i są jego inherentną częścią.

\begin{figure}[h]
  \centering
\includegraphics[scale=0.8]{../obrazy/fig:vm.png}
\caption{Wirtualizacja pozwala na uruchomienie licznych maszyn wirtualnych na jednej maszynie fizycznej z wykorzystaniem hipernadzorcy (w tym wypadku \textit{VMware}) \label{fig:vm}}
\end{figure}

Technologia wirtualizacji musi jednak zmierzyć się z pewnymi wyzwaniami. Architektura x86 nie była pierwotnie tworzona jako platforma dedykowana tym rozwiązaniom. Mechanizmy, które pozwalają na tworzenie maszyn wirtualnych na tego typu architekturze wymagają zmodyfikowanego systemu operacyjnego lub używają dodatkowego zestawu instrukcji, dostarczonego przez nowoczesne procesory. Z tego powodu wykorzystywanie wirtualizacji wiąże się z pewnym spadkiem wydajności. Choć według \citet{menon2005} został on w ostatnich latach zminimalizowany, wirtualne maszyny wciąż dostarczają tylko ułamek wydajności równoważnej fizycznej maszyny. Niektóre rodzaje hipernadzorców pozwalają na dostarczenie niemal natywnej wydajności procesora, jednak sedno problemu leży w wydajności procesów wejścia/wyjścia (\textit{input/output (I/O)}), którego spowolnienie może osiągać nawet 88\% w porównaniu do maszyny fizycznej. Ponadto, należy skupić się także na minimalizowaniu strat w przypadku uruchamianie wielu maszyn wirtualnych na jednym rdzeniu procesora. Ponieważ w danym momencie na jednym procesorze może być aktywna tylko jedna maszyna wirtualna, pozostałe z nich pozostają bezczynne i nie mogą odpowiadać na aktywność \textit{I/O}.

\subsubsection{Aspekty ekonomiczne}

\noindent
Przetwarzanie w chmurze stało się jedną z najszybciej rozwijających się gałęzi technologii informacyjnych. W 2009 r. sprzedaż w usługach chmurowych osiągnęła 56 miliardów dolarów \citep{smith2009}, podczas gdy Merryl Lynch przewidywał wzrost wartości rynku do 160 miliardów w 2011. Raport Gartner z 2016 roku \citep{anderson2016} wskazuje, że rynek platform BPaaS, SaaS, Paas oraz Iaas osiągnął w sumie wartość 734 miliardów dolarów, przy czym kolejne 111 miliardów zostanie w niego zainwestowana do 2020 r. Ponieważ uważa się, że technologia ta ma wszelkie argumenty, by fundamentalnie i na stałe zmienić sposób zarządzania zasobami informatycznymi przedsiębiorstwa, dyrektorzy i menedżerowie IT powinni nieustannie śledzić rozwój tej innowacji i dostosowywać do niej swoje długofalowe strategie.

Sukces \textit{cloud computingu} ściśle związany jest z korzyściami ekonomicznymi, które za sobą niesie. Rozwiązanie to pozwala na obniżenie kosztów obsługi, szybszą implementację i większą elastyczność w wykorzystywaniu różnych zestawów narzędzi. Wykorzystanie modeli \textit{pay-as-you-go} skutkuje także minimalizowaniem ryzyka nadmiernego gromadzenia zasobów (\textit{overprovisioning}), jak i ich niewystarczającej ilości (\textit{underprovisioning}). Według \citet{armburst2010} średnie wykorzystanie serwerów w centrach danych wynosi od 5\% do 20\%, co jest spójne z obliczeniami, według których największe obciążenie serwerów przekracza nawet dziesięciokrotnie średnią.  W tym modelu dodatkowe maszyny mogą zostać usunięte lub dodane w ciągu kilku minut (w przeciwieństwie do tygodni-miesięcy w przypadku klasycznego modelu). Efektem tego jest zbilansowana proporcja zapotrzebowania na sprzęt i faktycznego stanu posiadania, co minimalizuje koszty i pozwala uniknąć przeciążania serwerów (zjawisko \textit{overprovisioningu} i \textit{underprovisioningu} zostało zilustrowane na Rysunku \ref{fig:overp}.). Największe korzyści z tego rozwiązania mogą uzyskać te przedsiebiorstwa, które z wykorzystaniem analiz ekonometrycznych potrafią przewidzieć zapotrzebowanie na swoje usługi (tym bardziej jeśli zapotrzebowanie dynamicznie zmienia się na przykład w zależności od pory dnia lub online-owych kampanii reklamowych, jak ma to miejsce w przypadku e-commerce). Ponadto, ta cecha \textit{cloud computingu} ma silny wpływ na jego popularność wśród małych i średnich przedsiębiorstw, które są we wczesnym etapie rozwoju. Promuje ona innowacyjne rozwiązania i pozwala na łatwe znalezienie zasobów koniecznych do ich realizacji. Dlatego też nawet jeśli nabywanie serwerów w modelu \textit{pay-as-you-go} okazuje się droższe niż metoda tradycyjna, może to być atrakcyjniejszą alternatywą ze względu na możliwość szybkiego zwolnienia maszyn bez dodatkowych kosztów.

\begin{figure}[h]
  \centering
\includegraphics[scale=0.8]{../obrazy/fig:overp.png}
\caption{Tradycyjna architektura wymusza rzadkie zmiany, które skutkują częstym nadmiarem zasobów obliczeniowych. Może on być nawet pogłębiony przez nieoczekiwany spadek zapotrzebowania tuż po zakupieniu dodatkowej mocy obliczeniowej, co generuje dodatkowe koszty. \label{fig:overp}}
\end{figure}

Korzystając z gotowych rozwiązań oferowanych przez dostawców usług \textit{cloud computingowych} klienci mogą również liczyć na zdecydowanie wyższy poziom bezpieczeństwa niż dotychczas, ponieważ ich dane chronione są przez jedno, wysoce wyspecjalizowane w tym zakresie przedsiębiorstwo.

Tego rodzaju model jest także bardzo opłacalny dla dostawców, ponieważ sposób zarządzania tego rodzaju homogenicznym systemem może być optymalizowany. Według \citet{baun2011} konstruowanie wielkoskalowych rozwiązań chmurowych w tanich lokalizacjach pozwala na obniżenie kosztów elektryczności, połączenia sieciowego, operacji, oprogramowania i sprzętu nawet siedmiokrotnie.

Pojawienie się przetwarzania w chmurze miało stymulujący wpływ na gospodarkę, ale także stawia przed sobą wiele nowych wyzwań. W związku z tym, regulatorzy mają również istotną rolę do odegrania w wielu obszarach:
\begin{itemize}
\item bodźce do używania technologii chmurowych -- rządy muszą odegrać rolę w zachęcaniu wykorzystania \textit{cloud computingu} na przykład poprzez usuwanie zbędnych barier prawnych i regylacyjnych, ale również przez bycie wiodącym ich użytkownikiem i zachęcanie do partnerstw publiczno-prywatnych,
\item standardy -- jednym z wielu wyzwań stojących przed rozwojem chmury jest brak odpowiednich standardów w niektórych obszarach, jak również niedostateczne przyjęcie istniejących standardów. Rządy powinny zachęcać i wspierać rozwój otwartych standardów dla wsparcia biznesu, także poprzez współpracę z różnym instytutami normalizacyjnymi,
\item pomiary -- ilość dostępnych publicznie danych o tego typu przedsięwzięciach jest wciąż niewielka. Regulatorzy, w konsultacji z udziałowcami, powinni odegrać istotną rolę w identyfikowaniu stosownych kryteriów pomiaru wyników i oceny stanu tej gałęzi rynku.
\item kraje rozwijające się -- \textit{cloud computing} daje szansę korzystania ze swoich korzyści przy niskich kosztach dla organizacji i konsumentów pochodzących z krajów rozwijających się. Należy zapewnić odpowiednią infrastrukturę sieciową oraz zachęcać do adaptacji i wykorzystania nowych technologii w owych krajach tak, by mogły one doświadczyć wzrostu ekonomicznego, poprawić zdolności edukacyjne, ale również by umożliwić wolny przepływ informacji dla rozwoju społeczeństwa,
\item konkurencja i handel -- ustawodawcy muszą zadbać o to, by nie dopuścić do niekonkurencyjnych praktyk wynikających z dominacji rynkowej kilku niewielkich przedsiębiorstw. Jest to niezwykle istotne, ponieważ (przywołując również teorię \textit{cloud computingu} jako medium) gałęzie rynku, gdzie kluczowym czynnikiem jest ekonomia skali są szczególnie narażone na tego typu sytuację,
\item podatki -- wraz z migracją wielu centrów danych do chmury, należy rozważyć potencjalny wpływ na rozliczenia podatkowe związane z dostarczaniem tych usług. Szczególnie ważna jest to w przypadku przedsiębiorstw, które dotychczas miały niewielkie doświadczenie w transgranicznych transakcjach i rachunkowości,
\item bezpieczeństwo i zarządzanie ryzykiem -- należy wprowadzić regulacje, które pozwolą trafnie zdefiniować stojące przed \textit{cloud computingiem} wyzwania związane z bezpieczeństwem infrastruktury, w szczególności konieczne jest przeprowadzenie oceny ryzyka, zarządzanie bezpieczeństwem i w efekcie jego usprawnienie,
\item prywatność -- regulatorzy powinni zaadresować pojawiające się pytania dotyczące prywatności przechowywanych danych, takich jak: ,,prawa którego państwa stosuje się do danych przechowywanych w chmurze?'', ,,kto ma dostęp do przechowywanych danych?'', ,,w jakich sytuacjach dane mogą zostać udostępnione jednostkom rządowym?''.
\end{itemize}

\subsubsection{Wiodący dostawcy usług \textit{cloud computingowych}}

\noindent
Według najnowszego raportu ,,Magic Quadrant'' firmy Gartner \citep{leong2017}, liderem rynku pozostaje, niezmiennie od 2006 roku, Amazon Web Services, na kolejnych pozycjach plasują się Microsoft, IBM oraz Google (wyniki rankingu przedstawione zostały na Rysunku \ref{fig:mquad}.). Według \citet{forbes2017}, Amazon posiada 31\% udziału w rynku, Microsoft 11\%, podczas gdy IBM i Google odpowiednio 7\% i 5\%. Jednak, pomimo statusu lidera, Amazon musi czynić intensywne wysiłki by zachować swoją pozycję, ponieważ inni wielcy gracze rozwijają się w imponującym tempie, zdobywając coraz większy udział w rynku infrastruktury chmurowej. Microsoft w czwartym kwartale 2015 powiększył swoje obroty o 124\%, podczas gdy w drugim kwartale 2016 roku to Google odnotował największe tempo wzrostu (162\%). Należy jednocześnie zwrócić uwagę na fakt, że czterech największych graczy stopniowo zwiększa swój udział w rynku (z 51\% w czwartym kwartale 2015 do 54\% w drugim kwartale 2016), por. \citet{forbes2017}.

\begin{figure}[h]
  \centering
\includegraphics[scale=0.8]{../obrazy/fig:mquad.png}
\caption{\textit{Magic Quadrant} dla przedsiębiorstw oferujących usługi \textit{cloud computingowe} potwierdza status lidera AWS. Należy jednak rozpatrywać Microsoft oraz Google jako równorzędnych konkuretnów.\label{fig:mquad}}
\end{figure}

Amazon, pierwszy dostawca usług przetwarzania w chmurze, przechodził stopniową transformację od portalu oferującego książki do online'owego potentata, wykorzystując jednocześnie wysoce innowacyjne rozwiązania do osiągnięcia tego celu. Ta otwartość na innowacje była niejako wymuszona stopniem skomplikowania oferowanego serwisu. Wyświetlenie jednej strony zawierającej informacje o przedmiocie, rekomendacje oraz oceny wymagało licznych, skomplikowanych aplikacji. Ponadto, detaliczny charakter sprzedaży wymagał ciągłego monitorowania zapotrzebowania na zasoby obliczeniowe, ale również obserwowania popytu i wsparcia  procesów operacyjnych systemami informatycznymi (zarówno swoich, jak i partnerów biznesowych). Amazon wykorzystywał technologię wirtualizacji podobnie, jak wiele innych przedsiębiorstw obecnie, by automatycznie i dynamicznie przydzielać zasoby do aplikacji, które tego potrzebowały w danej chwili. Wysoki stopień automatyzacji, który osiągneli w tym zakresie pozwolił im na uruchomienie ich dwóch największych serwisów -  Amazon EC2 (Elastic Compute Cloud) oraz Amazon S3 (Simple Storage System). Amazon EC2 pozwala użytkownikom chmury uruchamiać serwery i zarządzać nimi w jednym z wielu umiejscowionych na różnych kontynentach centrów danych, podczas gdy Amazon S3 jest usługą służącą do przechowywania danych w chmurze. Usługi te miały początkowo zostać udostępnione wewnętrznie i dla partnerów biznesowych, jednak w ramach eksperymentu otwarto je również dla całego świata, gdzie spotkały się z ogromną popularnością, por. \citet{shroff2010}. Ostatecznie doprowadziło do decyzji o uruchomieniu biznesu \textit{cloud computingowego} w 2006 roku. Od tego czasu AWS znacznie rozszerzyło gamę oferowanych usług i dostarcza swoje usługi z wykorzystaniem centrów danych znajdujących się w Ameryce Północnej, Europie, Azji oraz Ameryce Południowej. Do ich silnych stron zaliczana jest duża liczba klientów, najszerszy wachlarz oferowanych usług oraz największe możliwości obliczeniowe, co doprowadziło do ich partnerstwa technologicznego z licznymi przedsiębiorstwami, które dostosowały swoje produkty do uruchamiania na platformie AWS. Z drugiej strony, wymaga to od początkującego użytkownika dużego nakładu czasu, by zaznajomić się z modelem działania i oferowanymi usługami tak, by mógł dostosować je do swoich potrzeb technologicznych.

Microsoft także coraz bardziej skupia się na dostarczaniu swoich możliwości technologicznych z wykorzystaniem usług chmurowych. Platforma przez nich Azure, pozwala na wynajęcie maszyn wirtualnych (z wykorzystaniem oprogramowania Hyper-V), przechowywanie obiektów w chmurze, jak również wiele innych możliwości IaaS oraz PaaS. Ponadto, Azure Marketplace oferuje oprogramowanie i usługi dostarczane przez inne podmioty. Co więcej, Microsoft błyskawicznie wprowadza nowe usługi i funkcjonalności, nakreślając wizję połączonych modeli IaaS oraz PaaS, które współdziałać będą z istniejącą infrastrukturą \textit{on-premises}.  Centra danych Microsoft (określane mianem ,,regionów'') znajdują się nie tylko w Ameryce Północnej i Południowej, Europie i Azji, ale także w Australii. Wartym odnotowania jest również fakt otwarcia się na technologie open source - gama tego typu rozwiązań wspieranych przez Azure wciąż się poszerza, ze szczególnym uwzględnieniem systemów operacyjnych Linux. Microsoft na pewno będzie również korzystał z istniejących kontaktów biznesowych oraz silnej marki, by promować swój produkt i osiągnąć jeszcze większy udział w rynku technologii chmurowych. Choć Azure daleko do stopnia rozwoju i dojrzałości prezentowanego przez AWS, wiele organizacji określa go jako wystarczająco rozbudowany, by móc w niego inwestować.

IBM, jeden z najstarszych koncernów informatycznych, również oferuje usługi \textit{cloud computingowe}. W celu przyspieszenia ekspansji na rynku, w 2013 roku IBM kupiło SoftLayer, niezależnego dostawcę usług chmurowych, by w efekcie ukształtować IBM Cloud Services Division. Przez kolejne lata następowała stopniowa migracja klientów z IBM SmartCloud Enterprise do SoftLayer. Obecnie oferują zarówno wirtualizowane serwery, jak i klasyczne dedykowane maszyny. Ponadto, istnieje także możliwość przechowywania obiektów poprzez zintegrowany CDN (\textit{Content Delivery Network}. Softlayer oferuje swoje usługi z wykorzystaniem centrów danych znajdujących się w Ameryce Północnej, Europie oraz Azji. Do zalet IBM Softlayer, podobnie jak w przypadku, Microsoft, można zaliczyć silną markę oraz liczne kontakty z klientami na całym świecie, co z pewnością pozwoli na rozwój w segmencie przedsiębiorstw. Należy jednak zaznaczyć, że historycznie Softlayer był głównie skupiony na klasycznym modelu dostarczania mocy obliczeniowej i nie rozróżnia na swoim portalu serwisów chmurowych i niechmurowych, brak jest także wielu możliwości technologicznych charakterystycznych dla przetwarzania w chmurze, oferowanych przez liderów rynku.

Google operuje na rynku PaaS pod 2008 roku (oferując swój App Engine), jednakże w modelu IaaS aktywni są dopiero od udostępnienia światu Google Compute Engine w 2013 roku. Obecnie w swojej ofercie łączą zarówno wymienione usługi, jak również wachlarz dodatkowych możliwości, takich jak możliwość przechowywania danych i Container Engine - usługę pozwalającą na konteneryzację, to znaczy uruchomienie aplikacji w wydzielonym konetenerze, ale bez konieczności emulowania warstwy sprzętowej i systemu operacyjnego. Centra danych Google znajdują się w Ameryce Północnej, Europie i Azji. Przez wiele lat prowadzenia własnego biznesu internetowego, Google nabyło szeroką ekspertyzę w projektowaniu tego typu rozwiązań. Niewątpliwą zaletą używania ich serwisu jest możliwość korzystania z ich osiągnięć technologicznych w zakresie wirtualizacji, wykorzystania kontenerów czy też narzędzi do zarządzania klastrami jak Kubernetes. Należy wszakże zaznaczyć, że zakres usług przez nich oferowany nie jest jeszcze tak szeroki jak w przypadku liderów rynku. Według \citet{leong2017}, pomimo szybkiego tempa wzrostu, Google wciąż nie rozwija się dostatecznie dynamicznie, by móc osiągnąć status lidera w tej gałęzi rynku informatycznego.

\clearpage

\section{Eksperymentalne pomiary efektywności kosztowej deep learning}

\noindent
Sukces głębokich sieci neuronowych przypisywany jest ich umiejętności reprezentacji danych wejściowych i odkrywania panujących w nich zależności z wykorzystaniem wielu ukrytych warstw \citep{lecun2015}. Kluczową rolę w ich budowaniu odegrało użycie procesorów graficznych, które pozwoliły na przyspieszenie uczenia tego typu architektur. Ich szerokie wykorzystanie spotkało się z reakcją ze strony dostawców usług \textit{cloud computingowych}, którzy wprowadzili do swoich ofert jednostki wyposażone w procesory graficzne i optymalizowane pod kątem obliczeń naukowych, wynajmując je po wyższych cenach wraz z obietnicą przyspieszenia operacji algebraicznych. Konfiguracje oferowane przez najpopularniejszych dostawców usług przetwarzania w chmurze przedstawione zostały w tabeli \ref{tab:gpu_units}. Spośród największych graczy, Amazon Web Services oraz Microsoft Azure oferują kompletne zestawy, optymalizowane pod kątem obliczeń naukowych (odpowiednio typy p2.* oraz N*), podczas gdy Google Cloud Platform w obecnie obowiązującej wersji beta pozwala na dołączanie kart graficznych do jednej z oferowanych maszyn. Ponadto, można zauważyć, że o ile jednostki podstawowe są bardzo podobne w przypadku AWS i Azure (p2.xlarge oraz NC6), o tyle wzrost dostępnej mocy obliczeniowej bardziej skomplikowanych jednostek jest znacznie szybszy w przypadku AWS. Wyłącznie w ramach Microsoft Azure są również oferowane jednostki nowszej generacji - NVIDIA Tesla M60.

Ponadto, obecnie do wyboru jest także szeroki wachlarz oprogramowania służacego do konstruowania sieci neuronowych, również z wykorzystaniem GPU. Zaliczają się do nich takie narzędzia jak Caffe (Uniwersytet w Berkeley, \citet{jia2014}), CNTK (Microsoft Research, \citet{yu2014}), MXNet (Distributed Machine Learning Community, \citet{chen2015}), Tensorflow (Google, \citet{abadi2016}), Torch \citep{collobert2011}, Theano (Uniwersytet w Montrealu, \citet{alrfou2016}) oraz Keras (ONEIROS, \citet{chollet2015}). Jednym z głównych zadań problemu uczenia głebokich sieci neuronowych jest znalezienie optymalnego zestawu wag w każdej z warstw sieci, co jest zadaniem, które może zostać sparalelizowane. Każde z wymienionych narzędzi wspiera przetwarzanie równoległe, zarówno z użyciem wielordzeniowych jednostek centralnych, jak i procesorów graficznych. Obliczenia na GPU wykonywane są z pomocą bibliotek dostarczonych przez NVIDIA - cuDNN \citep{chetlur2014}.
Praca \citet{shi2017} porównuje większość prezentowanych systemów oprogramowania ze sobą, przedstawiając ich silne i słabe strony w popularnych problemach \textit{deep learningowych}.

\noindent
\begin{tabular}
  {p{0.08\linewidth}
  p{0.15\linewidth}
  p{0.24\linewidth}
  p{0.08\linewidth}
  p{0.10\linewidth}
  p{0.20\linewidth}}
\toprule
 & Typ jednostki & Procesor & Rdzeni CPU & RAM & GPU \\
\midrule
AWS & p2.xlarge & Intel Xeon E5-2686v4 & 4 & 61 GB & NVIDIA K80 \\
    & p2.8xlarge & Intel Xeon E5-2686v4 & 32 & 488 GB & 8x NVIDIA K80 \\
    & p2.16xlarge & Intel Xeon E5-2686v4 & 64 & 732 GB & 16x NVIDIA K80 \\
\midrule
Azure & NC6 & Intel Xeon E5-2690v3 & 6 & 56 GB & NVIDIA K80 \\
      & NC12 & Intel Xeon E5-2690v3 & 12 & 112 GB & 2x NVIDIA K80 \\
      & NC24(r) & Intel Xeon E5-2690v3 & 24 & 224 GB & 4x NVIDIA K80 \\
      & NV6 & Intel Xeon E5-2690v3 & 6 & 56 GB & NVIDIA M60 \\
      & NV12 & Intel Xeon E5-2690v3 & 12 & 112 GB & 2x NVIDIA M60 \\
      & NV24(r) & Intel Xeon E5-2690v3 & 24 & 224 GB & 4x NVIDIA M60 \\
\midrule
GCP & N/D & N/D & N/D & N/D & NVIDIA K80 \\
\bottomrule
\label{tab:gpu_units}
\end{tabular}

Istnieje uzasadniona potrzeba pomiaru, na jak dużą oszczędność czasową może liczyć osoba decydująca się na wspomaganie obliczeń naukowych procesorami graficznymi oferowanymi przez platformy \textit{cloud computingowe} i czy wiąże się to z dodatkowymi kosztami. Ekperyment ma na celu pomiar czasu potrzebnego na proces generacji różnych architektur głębokich sieci neuronowych na różnych zbiorach danych i związanego z tym kosztu, wybranie zbioru decyzji niezdominowanych (optymalnych w sensie Pareto), a także dostarczenie narzędzia pozwalającego na przeprowadzanie własnych eksperymentów w tym zakresie.
Spośród przedstawionych w tabeli \ref{tab:gpu_units} konfiguracji wybrano jednostkę p2.xlarge oferowaną przez Amazon Web Services do porównania z podobnymi jednostkami bez procesorów graficznych ze względu na status lidera rynku i dojrzałość rozwiązań oferowanych przez tego dostawcę.
W eksperymencie wykorzystana została także biblioteka Keras, w oparciu o backend Tensorflow. Powodem tej decyzji jest fakt, że została zbudowana w celu ułatwienia przeprowadzania tego typu eksperymentów i oferuje wysoki poziom abstrakcji przy konfigurowaniu architektur bez względu na wykorzystaną u jej podstaw bibliotekę naukową (obecnie możliwe jest wykorzystanie Tensorflow, Theano oraz Deeplearning4j).

% co oferuja dostawcy (opisac hardware) + jaki wybralem
% jaki jest software + jaki wybralem
% dlaczego taki eksperyment
% co ma zbadac eksperyment
% jaki bedzie jego efekt
% jak go przeprowadze
% tabela ze specyfikacjami maszyn ktore ostatecznie zostaly wybrane do eksperymentu
% zbiorki danych opisac
% obrazki architektur

%

%tabela z czasami
\begin{tabular}{lllll}
\toprule
Zbiór danych & Architektura & Typ instancji & Czas (s) & Minimum funkcji straty \\
\midrule
cifar & custom & c4.2xlarge & $29.16\pm0.32$ & $0.0852$ \\
      & & p2.xlarge & $4.05\pm0.09$ & $0.0913$ \\
\midrule
      & kerasdef & c4.2xlarge & $23.20\pm0.53$ & $0.3513$ \\
      & & p2.xlarge & $4.39\pm0.14$ & $0.3391$ \\
\midrule
imdb & kerasdef & c4.2xlarge & $125.12\pm2.07$ & $0.0309$ \\
      & & p2.xlarge & $10.31\pm0.44$ & $0.0314$ \\
\midrule
      & lstmkerasdef & c4.2xlarge & $46.80\pm1.87$ & $0.0101$ \\
      & & p2.xlarge & $40.16\pm0.92$ & $0.0100$ \\
\midrule
mnist & custom & c4.2xlarge & $101.53\pm0.68$ & $0.1152$ \\
      & & p2.xlarge & $13.00\pm0.28$ & $0.1112$ \\
\midrule
      & kerasdef & c4.2xlarge & $89.55\pm1.30$ & $0.0352$ \\
      & & p2.xlarge & $13.26\pm8.96$ & $0.0369$ \\
\bottomrule
\label{tab:experiment_results}
\end{tabular}

\clearpage

\section{Efektywność kosztowa głębokich sieci neuronowych w chmurze}

\clearpage

\section{Podsumowanie pracy}
%podsumowanko

\clearpage

\begin{thebibliography}{99}
\setlength{\itemsep}{0pt}%
\bibitem[Abadi et al. (2016)]{abadi2016} Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., ... i Ghemawat, S. (2016). Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467
\bibitem[Akaike (1974)]{akaike1974} Akaike, H. (1974). A new look at the statistical model identification. IEEE transactions on automatic control, 19(6), 716-723
\bibitem[Al-Rhou et al. (2016)]{alrhou2016} Al-Rfou, R., Alain, G., Almahairi, A., Angermueller, C., Bahdanau, D., Ballas, N., ... i Bengio, Y. (2016). Theano: A Python framework for fast computation of mathematical expressions. arXiv preprint arXiv:1605.02688
\bibitem[Almeida i Silva (1990)]{almeida1990} Almeida, L. i Silva, F. (1990). Speeding up backpropagation. Adv Neural Comput, 151-158
\bibitem[Anderson i Warrilow (2016)]{anderson2016} Anderson, E. i Warrilow, M. (2016). Market Insight: Cloud Shift — The Transition of IT Spending From Traditional Systems to Cloud. Baza danych Gartner
%\bibitem[Andrychowicz et al. (2016)]{andrychowicz2016} Andrychowicz, M., Denil, M., Gomez, S., Hoffman, M. W., Pfau, D., Schaul, T. i de Freitas, N. (2016). Learning to learn by gradient descent by gradient descent. In Advances in Neural Information Processing Systems, s. 3981-3989
\bibitem[Antonopoulos i Gilliam (2010)]{antonopoulos2010} Antonopoulos, N. i Gillam, L. (2010). Cloud computing, 1st ed. London: Springer
\bibitem[Armburst et al. (2010)]{armburst2010} Armbrust, M., Stoica, I., Zaharia, M., Fox, A., Griffith, R., Joseph, A., Katz, R., Konwinski, A., Lee, G., Patterson, D. i Rabkin, A. (2010). A view of cloud computing. Communications of the ACM, 53(4), s. 50
\bibitem[Baun i Kunze (2011)]{baun2011} Baun, C. i Kunze, M. (2011). Cloud computing. 1st ed. Heidelberg [etc.]: Springer
\bibitem[Bengio i Delalleau (2011)]{bengio2011} Bengio, Y. i Delalleau, O. (2011, October). On the expressive power of deep architectures. In International Conference on Algorithmic Learning Theory (s. 18-36). Springer Berlin Heidelberg
\bibitem[Bengio et al. (2007a)]{bengio2007a} Bengio, Y., Lamblin, P., Popovici, D. i Larochelle, H. (2007a). Greedy layer-wise training of deep networks. Advances in neural information processing systems, 19, 153
\bibitem[Bengio i LeCun (2007b)]{bengio2007b} Bengio, Y. i LeCun, Y. (2007b). Scaling learning algorithms towards AI. Large-scale kernel machines, 34(5), 1-41
\bibitem[Buyya et al. (2009)]{buyya2009} Buyya, R., Yeo, C., Venugopal, S., Broberg, J. i Brandic, I. (2009). Cloud computing and emerging IT platforms: Vision, hype, and reality for delivering computing as the 5th utility. Future Generation Computer Systems, 25(6), s. 599-616
\bibitem[Carr (2008)]{carr2008} Carr, N. G. (2008). The big switch: Rewiring the world, from Edison to Google. WW Norton and Company
\bibitem[Calheiros et al. (2010)]{calheiros2010} Calheiros, R., Ranjan, R., Beloglazov, A., De Rose, C. i Buyya, R. (2010). CloudSim: a toolkit for modeling and simulation of cloud computing environments and evaluation of resource provisioning algorithms. Software: Practice and Experience, 41(1), s. 23-50
\bibitem[Chellapilla et al. (2006)]{chellapilla2006} Chellapilla, K., Puri, S. i Simard, P. (2006). High performance convolutional neural networks for document processing. Tenth International Workshop on Frontiers in Handwriting Recognition. Suvisoft
\bibitem[Chen et al. (2016)]{chen2016} Chen, L. C., Papandreou, G., Kokkinos, I., Murphy, K. i Yuille, A. L. (2016). Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. arXiv preprint arXiv:1606.00915
\bibitem[Chen et al. (2015)]{chen2015} Chen, T., Li, M., Li, Y., Lin, M., Wang, N., Wang, M., ... i Zhang, Z. (2015). Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems. arXiv preprint arXiv:1512.01274
\bibitem[Chetlur et al. (2014)]{chetlur2014} Chetlur, S., Woolley, C., Vandermersch, P., Cohen, J., Tran, J., Catanzaro, B. i Shelhamer, E. (2014). cudnn: Efficient primitives for deep learning. arXiv preprint arXiv:1410.0759
\bibitem[Chollet et al. (2015)]{chollet2015} Chollet, F. i inni (2015). Dostępne pod adresem: \url{https://github.com/fchollet/keras} (dostęp 6 czerwca 2017). GitHub
\bibitem[Ciresan et al. (2010)]{ciresan2010} Ciresan, D. C., Meier, U., Gambardella, L. M. i Schmidhuber, J. (2010). Deep big simple neural nets for handwritten digit recogntion. Neural Computation, 22(12):3207–3220.
\bibitem[Ciresan et al. (2011a)]{ciresan2011a} Ciresan, D. C., Meier, U., Masci, J., Gambardella, L. M. i Schmidhuber, J. (2011a). Flexible, high performance convolutional neural networks for image classification. International Joint Conference on Artificial Intelligence IJCAI, s. 1237–1242
\bibitem[Ciresan et al. (2011b)]{ciresan2011b} Ciresan, D. C., Meier, U., Masci, J. i Schmidhuber, J. (2011b). A committee of neural networks for traffic sign classification. International Joint Conference on Neural Networks (IJCNN), s. 1918–1921.
\bibitem[Ciresan et al. (2012)]{ciresan2012} Ciresan, D. C., Giusti, A., Gambardella, L. M. i Schmidhuber, J. (2012). Deep neural networks segment neuronal membranes in electron microscopy images. Advances in Neural Information Processing Systems (NIPS), s. 2852–2860
\bibitem[Ciresan i Schmidhuber (2013)]{ciresan2013} Ciresan, D. C. and Schmidhuber, J. (2013). Multi-column deep neural networks for offline handwritten Chinese character classification. Technical report, IDSIA. arXiv:1309.0261
\bibitem[Collobert et al. (2011)]{collobert2011} Collobert, R., Kavukcuoglu, K. i Farabet, C. (2011). Torch7: A matlab-like environment for machine learning. In BigLearn, NIPS Workshop (nr EPFL-CONF-192376)
\bibitem[Edwards (1993)]{edwards1993} Edwards, P. J. (1993). Synaptic weight noise during MLP learning enhances fault-tolerance, generalisation and learning trajectory
\bibitem[Forbes.com (2017)]{forbes2017} Forbes.com. (2017). Amazon Continues To Gain Share In Cloud Infrastructure Services Market. Dostępne pod adresem: \url{https://www.forbes.com/sites/greatspeculations/2016/08/17/amazon-continues-to-gain-share-in-cloud-infrastructure-services-market/#2277485215b8} (dostęp 11 marca 2017)
\bibitem[Fukushima (1975)]{fukushima1975} Fukushima, K. (1975). Cognitron: A self-organizing multilayered neural network. Biological cybernetics, 20(3-4), 121-136
\bibitem[Fukushima (1980)]{fukushima1980} Fukushima, K. (1980). Neocognitron: A self-organizing neural network for a mechanism of pattern
recognition unaffected by shift in position. Biological Cybernetics, 36(4):193–202
\bibitem[Girshick et al. (2014)]{girschick2014} Girshick, R., Donahue, J., Darrell, T. i Malik, J. (2014). Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, s. 580-587
\bibitem[Github (2017)]{github2017} GitHub. (2017). Benchmarks for popular convolutional neural network models on CPU and different GPUs, with and without cuDNN. Dostępne pod adresem: \url{https://github.com/jcjohnson/cnn-benchmarks} (dostęp 14 marca 2017)
\bibitem[Goldberg (1974)]{goldberg1974} Goldberg, R. P. (1974). Survey of virtual machine research. IEEE Comput Mag 7(6):34–45
\bibitem[Goodfellow et al. (2016)]{goodfellow2016} Goodfellow, I., Bengio, Y. i Courville, A. (2016). Deep Learning. MIT Press
\bibitem[Graves i Schmidhuber (2005)]{graves2005} Graves, A. i Schmidhuber, J. (2005). Framewise phoneme classification with bidirectional LSTM and other neural network architectures. Neural Networks, 18(5), 602-610
\bibitem[Gysel (2016)]{gysel2016} Gysel P. M. (2016). Ristretto: Hardware-Oriented Approximation of Convolutional Neural Networks. arXiv preprint arXiv:1605.06402
\bibitem[Hacker Noon (2017)]{hackernoon2017} Hacker Noon. (2017). GPUs and Kubernetes for Deep Learning. Dostępne pod adresem: \url{https://hackernoon.com/gpus-kubernetes-for-deep-learning-part-1-3-d8eebe0dd6fe#.jn1xjvpd2} (dostęp 14 marca 2017)
%\bibitem[Han et al. (2015)]{han2015} Han, S., Mao, H. i Dally, W. J. (2015). Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149
\bibitem[Han et al. (2016)]{han2016} Han, S., Liu, X., Mao, H., Pu, J., Pedram, A., Horowitz, M. A. i Dally, W. J. (2016) EIE: Efficient Inference Engine on Compressed Deep Neural Network. arXiv preprint arXiv:1602.01528, 2016a
\bibitem[Hanson (1990)]{hanson1990} Hanson, S. J. (1990). A stochastic version of the delta rule. Physica D: Nonlinear Phenomena, 42(1-3), 265-272
\bibitem[Hastie i Tibshirani (1990)]{hastie1990} Hastie, T. J. i Tibshirani, R. J. (1990). Generalized additive models (tom 43). CRC press
\bibitem[Hebb (1949)]{hebb1949} Hebb, D.O. (1949). The Organization of Behavior. New York: Wiley and Sons
\bibitem[Hecht-Nielsen (1988)]{hecht1988} Hecht-Nielsen, R. (1988). Theory of the backpropagation neural network. Neural Networks, 1, s. 445
\bibitem[Hinton i Sejnowski (1986)]{hinton1986} Hinton, G. E. i Sejnowski, T. J. (1986). Learning and relearning in Boltzmann Machines. Parallel Distributed Processing, Rumelhart et al, Eds
\bibitem[Hinton i Van Camp (1993)]{hinton1993} Hinton, G. E. i Van Camp, D. (1993, August). Keeping the neural networks simple by minimizing the description length of the weights. Proceedings of the sixth annual conference on Computational learning theory (s. 5-13). ACM
\bibitem[Hinton (2006a)]{hinton2006a} Hinton, G. (2006a). Reducing the Dimensionality of Data with Neural Networks. Science, 313(5786), s. 504-507
\bibitem[Hinton et al. (2006b)]{hinton2006b} Hinton, G., Osindero, S. i Teh, Y. (2006b). A Fast Learning Algorithm for Deep Belief Nets. Neural Computation, 18(7), s. 1527-1554
\bibitem[Hinton et al. (2012)]{hinton2012} Hinton, G., Deng, L., Yu, D., Dahl, G., Mohamed, A., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T. i Kingsbury, B. (2012). Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups. IEEE Signal Processing Magazine, 29(6), s. 82-97
\bibitem[Hochreiter (1991)]{hochreiter1991} Hochreiter, S. (1991). Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, Institut für Informatik, Lehrstuhl Prof. Brauer, Technische Universität München. Advisor: J. Schmidhuber
\bibitem[Igel i Husken (2003)]{igel2003} Igel, C. i Hüsken, M. (2003). Empirical evaluation of the improved Rprop learning algorithms. Neurocomputing, 50, 105-123
\bibitem[Ivakhnenko i Lapa (1965)]{ivakhnenko1965} Ivakhnenko, A. G. i Lapa, V. G. (1965). Cybernetic Predicting Devices. CCM Information Corporation
\bibitem[Ivakhnenko i Lapa (1967)]{ivakhnenko1967} Ivakhnenko, A. G., Lapa, V. G., and McDonough, R. N. (1967). Cybernetics and forecasting techniques. American Elsevier, NY
\bibitem[Ivakhnenko i Lapa (1968)]{ivakhnenko1968} Ivakhnenko, A. G. (1968). The group method of data handling-a rival of the method of stochastic approximation. Soviet Automatic Control, 13(3), 43-55
\bibitem[Jacobs (1989)]{jacobs1989} Jacobs, R. A. (1988). Increased rates of convergence through learning rate adaptation. Neural networks, 1(4), 295-307
\bibitem[Jain i Seung (2009)]{jain2009} Jain, V. i Seung, S. (2009). Natural image denoising with convolutional networks. Koller, D., Schuurmans, D., Bengio, Y., and Bottou, L., editors, Advances in Neural Information Processing Systems (NIPS) 21, s. 769–776. Curran Associates, Inc.
\bibitem[Jermain et al. (2015)]{jermain2015} Jermain, C. L., Rowlands, G. E., Buhrman, R. A., i Ralph, D. C. (2015). GPU-accelerated micromagnetic simulations using cloud computing. arXiv preprint arXiv:1505.01207
\bibitem[Jia et al. (2014)]{jia2014} Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., ... i Darrell, T. (2014). Caffe: Convolutional architecture for fast feature embedding. Proceedings of the 22nd ACM international conference on Multimedia (s. 675-678). ACM
\bibitem[Kleinrock (2005)]{kleinrock2005} Kleinrock, L., A Vision for the Internet. ST Journal for Research, tom 2, nr 1, s. 4–5
\bibitem[Krizhevsky et al. (2012)]{krizhevsky2012} Krizhevsky, A., Sutskever, I. i Hinton, G. (2012). Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systemsk, s. 1097-1105
\bibitem[LeCun et al. (1989)]{lecun1989a} LeCun, Y., Denker, J. S., Solla, S. A., Howard, R. E., Jackel, L. D. (1989, November). Optimal brain damage. NIPs (tom 2, s. 598-605).
\bibitem[LeCun et al. (1989)]{lecun1989b} LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W. i Jackel, L. D. (1989). Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4), 541-551
\bibitem[LeCun, Bengio i Hinton (2015)]{lecun2015} LeCun, Y., Bengio, Y. i Hinton, G. (2015). Deep learning. Nature, 521(7553), s.436-444
\bibitem[Leong et al. (2017)]{leong2017} Leong, L., Petri, G., Gill, B. i Dorosh, M. (2017). Magic Quadrant for Cloud Infrastructure as a Service. Gartner.com. Dostępne pod adresem: \url{https://www.gartner.com/doc/reprints?id=1-2G2O5FC&ct=150519} (dostęp 11 marca 2017)
\bibitem[Linnainmaa (1970)]{linnainmaa1970} Linnainmaa, S. (1970). The representation of the cumulative rounding error of an algorithm as a
Taylor expansion of the local rounding errors. Praca magisterska, Uniwersytet w Helsinkach
\bibitem[Litvinenko (2014)]{litvinenko2014} Litvinenko, N. (2014). Using of GPUs for cluster analysis of large data by K-means method arXiv preprint arXiv:1402.3788
\bibitem[Mackay (1992)]{mackay1992} MacKay, D. J. (1992). A practical Bayesian framework for backpropagation networks. Neural computation, 4(3), 448-472
\bibitem[McCulloch i Pitts (1943)]{mcculloch1943} McCulloch, W. S. i Pitts, W. (1943). A logical calculus of the ideas immanent in nervous activity. The bulletin of mathematical biophysics, 5(4), s. 115-133
\bibitem[Medium (2017a)]{medium2017a} Medium (2017). TensorFlow 1.0 is here. Let’s do some Deep Learning on the Amazon Cloud! – Sigmoidal. Dostępne pod adresem: \url{https://medium.com/sigmoidal/tensorflow-1-0-is-here-lets-do-some-deep-learning-on-the-amazon-cloud-9234eab31fa5#.vyxzk4bwk} (dostęp 14 marca 2017)
\bibitem[Medium (2017b)]{medium2017b} Medium (2017). Keras with GPU on Amazon EC2 – a step-by-step instruction. Dostępne pod adresem: \url{https://medium.com/@mateuszsieniawski/keras-with-gpu-on-amazon-ec2-a-step-by-step-instruction-4f90364e49ac#.i2rxriv4g} (dostęp 14 marca 2017)
\bibitem[Mell i Grance (2011)]{mell2011} Mell, P. i Grance T. (2011). The NIST definition of cloud computing
\bibitem[Menon et al. (2005)]{menon2005} Menon, A., Santos, J., Turner, Y., Janakiraman, G. i Zwaenepoel, W. (2005). Diagnosing performance overheads in the xen virtual machine environment. Proceedings of the 1st ACM/USENIX international conference on Virtual execution environments - VEE '05
\bibitem[Minsky i Papert (1969)]{minsky1969} Minsky, M. i Papert, S. (1969). Perceptrons. Cambridge, MA: MIT Press
\bibitem[Misra et al. (2016)]{misra2016} Misra, I., Lawrence Zitnick, C., Mitchell, M. i Girshick, R. (2016). Seeing through the Human Reporting Bias: Visual Classifiers from Noisy Human-Centric Labels. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (s. 2930-2939)
\bibitem[Mnih et al. (2013)]{mnih2013} Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D. i Riedmiller, M. (2013). Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602
\bibitem[Moody (1991)]{moody1991} Moody, J. E. (1991, December). The effective number of parameters: An analysis of generalization and regularization in nonlinear learning systems. In NIPS (tom 4, s. 847-854)
\bibitem[Moody i Utans (1994)]{moody1994} Moody, J., i Utans, J. (1994). Architecture selection strategies for neural networks: Application to corporate bond rating prediction. In Neural networks in the capital markets (pp. 277-300). John Wiley \& Sons.
\bibitem[Mustafa et al. (2015)]{mustafa2015} Mustafa, S., Nazir, B., Hayat, A., Khan, A. i Madani, S. (2015). Resource management in cloud computing: Taxonomy, prospects, and challenges. Computers and Electrical Engineering, 47, s. 186-203
\bibitem[Nowlan i Hinton (1992)]{nowlan1992} Nowlan, S. J. i Hinton, G. E. (1992). Simplifying neural networks by soft weight-sharing. Neural computation, 4(4), 473-493
\bibitem[OECD (2014)]{oecd2014} OECD (2014). Cloud Computing: The Concept, Impacts and the Role of Government Policy. OECD Digital Economy Papers, No. 240, OECD Publishing, Paris
\bibitem[Oh i Jung (2004)]{oh2004} Oh, K. S. i Jung, K. (2004). GPU implementation of neural networks. Pattern Recognition, 37(6), 1311-1314
\bibitem[Ranzato et al. (2006)]{ranzato2006} Ranzato, M., Poultney, C., Chopra, S. i LeCun, Y. (2006). Efficient learning of sparse representations with an energy-based model. In et al., J. P., editor, Advances in Neural Information Processing Systems (NIPS 2006). MIT Press
\bibitem[Ranzato et al. (2007)]{ranzato2007} Ranzato, M. A., Huang, F., Boureau, Y. i LeCun, Y. (2007). Unsupervised learning of invariant feature hierarchies with applications to object recognition. Proc. Computer Vision and Pattern Recognition Conference (CVPR’07), s. 1–8. IEEE Press
\bibitem[Riedmiller i Braun (1993)]{riedmiller1993} Riedmiller, M. i Braun, H. (1993). A direct adaptive method for faster backpropagation learning: The RPROP algorithm. In Neural Networks, 1993., IEEE International Conference on (s. 586-591). IEEE
\bibitem[Rosenblatt (1957)]{rosenblatt1957} Rosenblatt, F. (1957). The Perceptron--a perceiving and recognizing automaton. Report 85-460-1, Cornell Aeronautical Laboratory
\bibitem[Rumelhart et al. (1986)]{rumelhart1986} Rumelhart, D. E., Hinton, G. E. i Williams, R. J. (1986). Learning representations by back-propagating errors. Nature, 323, 533-536
\bibitem[Schuster (1997)]{schuster1997} Schuster, M. i Paliwal, K. K. (1997). Bidirectional recurrent neural networks. IEEE Transactions on Signal Processing, 45(11), s. 2673-2681
\bibitem[Schmidhuber i Hochreiter (1997)]{schmidhuber1997} Schmidhuber, J. i Hochreiter, S. (1997). Long short-term memory. Neural Comput, 9(8), s. 1735-1780
\bibitem[Schmidhuber (2015)]{schmidhuber2015} Schmidhuber, J. (2015). Deep learning in neural networks: An overview. Neural networks, 61, 85-117
\bibitem[Shetty (2016)]{shetty2016} Shetty, S. (2016).  Application of Convolutional Neural Network for Image Classification on Pascal VOC Challenge 2012 dataset. arXiv preprint arXiv:1607.03785
\bibitem[Shroff (2010)]{shroff2010} Shroff, G. (2010). Enterprise cloud computing. 1st ed. Cambridge: Cambridge University Press
\bibitem[Smith et al. (2009)]{smith2009} Smith D. M. et al. (2009). Hype Cycle for Cloud Computing (ID: G00168780). Baza danych Gartner
\bibitem[Smolensky (1986)]{smolensky1986} Smolensky, P. (1986). Parallel distributed processing: Explorations in the microstructure of cognition, tom 1. Information Processing in Dynamical Systems: Foundations of Harmony Theory. MIT Press, Cambridge, MA, USA, 15, 18
\bibitem[Srivastava et al. (2014)]{srivastava2014} Srivastava, N., Hinton, G. E., Krizhevsky, A., Sutskever, I. i Salakhutdinov, R. (2014). Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1), s. 1929-1958
\bibitem[Strom (2015)]{strom2015} Strom, N. (2015). Scalable distributed DNN training using commodity GPU cloud computing. INTERSPEECH (tom 7, s. 10)
\bibitem[Szegedy (2014)]{szegedy2014} Szegedy, C. et al. (2014). Going deeper with convolutions. arXiv preprint arXiv:1409.4842
\bibitem[Tieleman i Hinton (2012)]{tieleman2012} Tieleman, T. i Hinton, G. (2012). Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2)
\bibitem[Touretzky i Hinton (1985)]{touretzky1985} Touretzky, D. S. i Hinton, G. E. (1985, August). Symbols among the neurons: Details of a connectionist inference architecture. In IJCAI (tom 85, s. 238-243)
\bibitem[Vanhoucke et al. (2011)]{vanhoucke2011} Vanhoucke, V., Senior, A. i Mao, M. Z. (2011). Improving the speed of neural networks on CPUs. In Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop (tom 1, s. 4)
\bibitem[Wan (1993)]{wan1993} Wan, E.A., 1993. Time series prediction by using a connectionist network with internal delay lines. Santa Fe Institute Studies in the sciences of complexity-proceedings volume (tom 15, s. 195-195). Addison-Wesley publishing co.
\bibitem[Ward i Baker (2013)]{ward2013} Ward, J.S. i Barker, A. (2013). A Cloud Computing Survey: Developments and Future Trends in Infrastructure as a Service Computing
\bibitem[Wenge et al. (1992)]{weng1992} Weng, J., Ahuja, N. i Huang, T. S. (1992, June). Cresceptron: a self-organizing neural network which grows adaptively. In Neural Networks, 1992. IJCNN., International Joint Conference on (Vol. 1, pp. 576-581). IEEE
\bibitem[Widrow i Hoff (1960)]{widrow1960} Widrow, B. i Hoff, M. E. (1960, August). Adaptive switching circuits. In IRE WESCON convention record (tom 4, nr 1, s. 96-104)
\bibitem[Yang et al. (2009)]{yang2009} Yang, M., Ji, S., Xu, W., Wang, J., Lv, F., Yu, K., Gong, Y., Dikmen, M., Lin, D. J. i Huang, T. S. (2009). Detecting human actions in surveillance videos. In TREC Video Retrieval Evaluation Workshop
\bibitem[Yu et al. 2014]{yu2014} Yu, D., Eversole, A., Seltzer, M., Yao, K., Huang, Z., Guenter, B., ... i Droppo, J. (2014). An introduction to computational networks and the computational network toolkit. Microsoft Technical Report MSR-TR-2014–112
\bibitem[Zhang et al. (2010)]{zhang2010} Zhang, Q., Cheng, L. i Boutaba, R. (2010). Cloud computing: state-of-the-art and research challenges. Journal of Internet Services and Applications, 1(1), s. 7-18

\end{thebibliography}
\clearpage

\listoffigures

\clearpage

\listoftables

\clearpage

\end{document}
